{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe5a748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 1\n",
    "\n",
    "import os, re, json, logging, hashlib, difflib\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI, RateLimitError\n",
    "import backoff\n",
    "# change by stephen2\n",
    "# === GPT Model Selection ===\n",
    "# Default model used by all GPT calls (Stage 1 & 2).\n",
    "# You can change this to \"gpt-4o\" or another snapshot if desired.\n",
    "OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "os.environ[\"OPENAI_MODEL\"] = OPENAI_MODEL\n",
    "print(f\"[config] GPT model set to: {OPENAI_MODEL}\")\n",
    "\n",
    "# Optional: warn if API key missing\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"[warning] OPENAI_API_KEY not found — GPT calls will fail unless it’s set.\")\n",
    "\n",
    "# === Initialize OpenAI client ===\n",
    "client = OpenAI()\n",
    "\n",
    "# Set variables:\n",
    "base_dir = Path(__file__).parent if \"__file__\" in globals() else Path.cwd()\n",
    "# ---- SET input_filename TO THE NAME OF THE PROCUREMENT SPREADSHEET YOU ARE ASSESSING !!!!!!!!!!!!!!!!!!!!!!!!----\n",
    "input_filename  = base_dir / \"input.csv\"\n",
    "# Base name for outputs\n",
    "base_filename = Path(input_filename).stem\n",
    "output_dir = base_dir / \"output\"\n",
    "\n",
    "# Set the input worksheet file type. See File Preparation section of documentation\n",
    "# Choose: \"auto\" | \"type01\" | \"type02\" | \"type03\"\n",
    "file_type = \"auto\"\n",
    "\n",
    "# START_MODE SWITCH to run full Notebook or resume part way through the process.\n",
    "# So, for example, you could review the ingredients and/or category output file, make edits and then\n",
    "# restart the process using your edited file, if you set START_MODE to \"after_ingredients\" or \"after_categories\"\n",
    "# Start at: \"full\" | \"after_ingredients\" | \"after_categories\"\n",
    "START_MODE = \"full\"\n",
    "\n",
    "# Optionally adjustable to eliminate ingredients that are a very minor percent of total ingredient weight\n",
    "CATEGORY_PERCENT_MIN = 2.1\n",
    "\n",
    "# factors.csv is the file with all the conversion factors from weight to impacts\n",
    "# foodcategories.json is a file that contains prior ingredient:category combinations \n",
    "# that can be checked for use, avoiding need for calls to AI if the ingredient is already present \n",
    "factors_filename    = base_dir / \"factors.csv\"\n",
    "categories_filename = base_dir / \"foodcategories.json\"\n",
    "\n",
    "# Session epoch for overwrite policy (v100g)\n",
    "import time\n",
    "_V100_SESSION_START = globals().get(\"_V100_SESSION_START\")\n",
    "if _V100_SESSION_START is None:\n",
    "    _V100_SESSION_START = time.time()\n",
    "    globals()[\"_V100_SESSION_START\"] = _V100_SESSION_START\n",
    "    \n",
    "# === Cell 1: Purpose: Set core filenames, modes, and global switches; verify required assets.\n",
    "# Inputs:\n",
    "#   - input_filename (user-specified CSV file), factors.csv (for ALLOWED_CATEGORIES), foodcategories.json.\n",
    "#   - constants and dictionaries (UOM_TO_LBS, CATEGORY_WEIGHT_MULTIPLIERS, etc.)\n",
    "# Outputs:\n",
    "#   - Global variables: BASE, START_MODE, CATEGORY_SOURCE, USE_CATEGORY_WEIGHT_MULTIPLIERS, etc.\n",
    "# Outputs: In-memory globals (e.g., ALLOWED_CATEGORIES, FOODCATS) used by later cells.\n",
    "#   - Logging configuration and printout of active settings.\n",
    "# Key processes:\n",
    "# • Imports core libraries and configures logging (INFO by default).\n",
    "# • Reads/sets: input_filename, file_type ('auto'|'type01'|'type02'|'type03'), and base_filename.\n",
    "# • Sets high-level run control START_MODE ('full' | 'after_ingredients' | 'after_categories').\n",
    "# • Loads foodcategories.json (exact name → category map) preserving insertion order.\n",
    "# • Loads factors.csv and composes ALLOWED_CATEGORIES (the “closed set” used by GPT).\n",
    "#\n",
    "# Notes:\n",
    "# • For OpenAI to be used later, the environment variable OPENAI_API_KEY must be set before running Cell 5/6.\n",
    "# • Changing input_filename or START_MODE here controls what the final Runner (Cell 9) will do.\n",
    "\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "\n",
    "# some different files being used for input_filename in testing\n",
    "# \"hh100g.csv\"\n",
    "# \"HH1.csv\"\n",
    "# \"testfoods80.csv\"\n",
    "# \"usfoods.csv\"\n",
    "# \"in1b_fpoint02.csv\"      # 213k\n",
    "# \"in2b_usfoods02.csv\"     # 226k\n",
    "# \"in2b_usfoods26.csv\"     # 15k\n",
    "# \"in3_jpaige01.csv\"       # 7k\n",
    "# \"in4b_jpolep01.csv\"      # 19k\n",
    "# \"in5b_ncsea01.csv\"       # 15k\n",
    "# \"in6b_jamix01.csv\"       # 23k\n",
    "# \"baldor100a.csv\"         # 40k\n",
    "# \"HH112in.csv\"\n",
    "# baldor103f.csv\n",
    "\n",
    "# Factor units hint (only used for output conversions in Stage 3)\n",
    "FACTOR_UNITS = {\"co2\":\"lbs_co2\"}  # change to \"kg_co2\" if your co2 factors are per kg\n",
    "\n",
    "def _load_foodcategories(json_path: Path) -> Dict[str, str]:\n",
    "    if not Path(json_path).exists():\n",
    "        logging.warning(\"No foodcategories.json found — using empty mapping.\")\n",
    "        return {}\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    mapping = {}\n",
    "    if isinstance(data, dict):\n",
    "        for k,v in data.items():\n",
    "            mapping[str(k).strip().lower()] = str(v).strip()\n",
    "    elif isinstance(data, list):\n",
    "        for rule in data:\n",
    "            if isinstance(rule, dict) and 'from' in rule and 'to' in rule:\n",
    "                cat = str(rule['to']).strip()\n",
    "                for src in rule['from']:\n",
    "                    mapping[str(src).strip().lower()] = cat\n",
    "    return mapping\n",
    "\n",
    "FOODCATS = _load_foodcategories(categories_filename)\n",
    "\n",
    "# Load category list from factors.csv\n",
    "if not Path(factors_filename).exists():\n",
    "    raise FileNotFoundError(f\"factors file not found: {factors_filename}\")\n",
    "_factors_df = pd.read_csv(factors_filename, encoding=\"utf-8\")\n",
    "if \"category\" not in _factors_df.columns:\n",
    "    raise ValueError(\"factors.csv must include a 'category' column\")\n",
    "FACTORS_CATEGORY_SET = set(str(x).strip() for x in _factors_df[\"category\"].dropna().unique())\n",
    "\n",
    "# Designed to help ensure that Food Categories selected for ingredients matches factors in factors.csv\n",
    "ALLOWED_CATEGORIES = sorted(FACTORS_CATEGORY_SET)\n",
    "\n",
    "# If worksheet does NOT show real weights in lbs for products with quantities expressed in fluid volumes such as fl oz \n",
    "# or pint, quart, gallon, ml, liter then set this to True in order for weights to be properly calculated. The multipliers\n",
    "# here convert from the default, weight of water, to weight of products that may be lighter or heavier than water.\n",
    "# --- Configuration Toggles and Data ---\n",
    "USE_CATEGORY_WEIGHT_MULTIPLIERS = False\n",
    "\n",
    "# These values represent the approximate specific gravity of the food, \n",
    "# which is the ratio of the food's density to the density of water (1.0).\n",
    "CATEGORY_WEIGHT_MULTIPLIERS = {\n",
    "    \"skim milk\": 1.034,  # Heavier due to solids, less fat\n",
    "    \"dairy milk\": 1.030, # Slightly lighter than skim milk\n",
    "    \"buttermilk\": 1.032, # Similar to skim milk\n",
    "    \"soy milk\": 1.020,\n",
    "    \"oat milk\": 1.015,\n",
    "    \"almond milk\": 1.010,\n",
    "    \"rice milk\": 1.008,\n",
    "    \"cream\": 0.995,      # Lighter than water due to high fat content\n",
    "    \"olive oil\": 0.915,  # All pure oils are less dense than water\n",
    "    \"palm oil\": 0.890,\n",
    "    \"rapeseed/canola oil\": 0.915,\n",
    "    \"soybean oil\": 0.920,\n",
    "    \"sunflower oil\": 0.918,\n",
    "    \"vegetable oils\": 0.925  # General range for mixed oils\n",
    "}\n",
    "\n",
    "print(f\"Updated multipliers to reflect Specific Gravity (Weight relative to Water = 1.0).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61924d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 2: Utilities ===\n",
    "# Purpose: Provide shared helper functions used throughout the FoodImpacts workflow.\n",
    "# Includes:\n",
    "#   - safe_read_csv: fault-tolerant CSV loading with encoding detection\n",
    "#   - parse_qty_unit: extract numeric quantity and unit of measure from product strings\n",
    "#   - to_lbs: convert quantity-unit pairs to weight in pounds\n",
    "#   - _norm: text normalization helper for category matching\n",
    "# Inputs: function arguments (varies)\n",
    "# Outputs: function return values; no direct file writes.\n",
    "# Behavior:\n",
    "#   Supports robust data ingestion, normalization, and measurement conversion.\n",
    "\n",
    "UOM_TO_LBS = {\n",
    "    # --- Weight/Mass Units (Exact Conversions) ---\n",
    "    \"LB\": 1.0, \"LBS\": 1.0, \"POUND\": 1.0, \"POUNDS\": 1.0,\n",
    "    \"OZ\": 1/16.0, \"OUNCE\": 1/16.0, \"OUNCES\": 1/16.0,\n",
    "    \"KG\": 2.20462262, \"KGS\": 2.20462262, \"KILOGRAM\": 2.20462262,\n",
    "    \"G\": 0.00220462, \"GRAM\": 0.00220462, \"GRAMS\": 0.00220462,\n",
    "\n",
    "    # --- Imperial Volume Units (Water Density Conversion to LBS) ---\n",
    "    # 1 US Gallon of water = 8.345 LBS\n",
    "    \"GAL\": 8.345, \"GALLON\": 8.345, \"GALLONS\": 8.345,\n",
    "    # 1 US Quart of water = 2.086 LBS (8.345 / 4)\n",
    "    \"QT\": 2.086, \"QUART\": 2.086, \"QUARTS\": 2.086,\n",
    "    # 1 US Fluid Ounce of water = 0.065 LBS (8.345 / 128)\n",
    "    \"FLUID OZ\": 0.065, \"FLOZ\": 0.065,\n",
    "\n",
    "    # --- Metric Volume Units (Water Density Conversion to LBS) ---\n",
    "    # 1 Liter of water = 1 KG = 2.2046 LBS\n",
    "    \"L\": 2.20462262, \"LITER\": 2.20462262, \"LITERS\": 2.20462262,\n",
    "    # 1 Milliliter of water = 1 Gram = 0.00220462 LBS\n",
    "    \"ML\": 0.00220462, \"MILLILITER\": 0.00220462, \"MILLILITERS\": 0.00220462\n",
    "}\n",
    "\n",
    "def to_lbs(qty: float, uom: str):\n",
    "    if pd.isna(qty) or pd.isna(uom): return None\n",
    "    key = str(uom).strip().upper()\n",
    "    factor = UOM_TO_LBS.get(key)\n",
    "    return float(qty) * factor if factor is not None and not np.isnan(factor) else None\n",
    "\n",
    "def parse_qty_unit(text: str):\n",
    "    if not isinstance(text, str): return (None, None)\n",
    "    m = re.search(r\"(?P<qty>\\d+(?:[\\.,]\\d+)?)\\s*(?P<unit>[A-Za-z]+)\\b\", text)\n",
    "    if not m: return (None, None)\n",
    "    try:\n",
    "        qty = float(m.group('qty').replace(',', '.'))\n",
    "    except Exception:\n",
    "        qty = None\n",
    "    unit = m.group('unit').upper()\n",
    "    return qty, unit\n",
    "\n",
    "def norm_text(s: str) -> str:\n",
    "    s = str(s or \"\").lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s/\\-]\", \" \", s)\n",
    "    s = s.replace(\"-\", \" \")\n",
    "    s = \" \".join(s.split())\n",
    "    return s\n",
    "\n",
    "def _normalize_colnames(cols):\n",
    "    out = []\n",
    "    for c in cols:\n",
    "        c2 = str(c).replace(\"\\u00A0\",\" \").strip().lower()\n",
    "        c2 = re.sub(r\"[^\\w]+\", \"_\", c2)\n",
    "        c2 = re.sub(r\"_+\", \"_\", c2).strip(\"_\")\n",
    "        out.append(c2)\n",
    "    return out\n",
    "\n",
    "def _try_read_csv(path):\n",
    "    for enc in (\"utf-8-sig\",\"utf-8\",\"latin1\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def safe_read_csv(path):\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    df = _try_read_csv(path)\n",
    "    df.columns = _normalize_colnames(df.columns)\n",
    "    return df\n",
    "\n",
    "def save_csv(df, path: str):\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8\")\n",
    "    logging.info(f\"Wrote: {path} ({len(df):,} rows)\")\n",
    "\n",
    "def load_if_exists(path: str):\n",
    "    p = Path(path)\n",
    "    if p.exists():\n",
    "        logging.info(f\"Loading existing file: {p}\")\n",
    "        return safe_read_csv(p)\n",
    "    return None\n",
    "\n",
    "def stable_hash(text: str) -> str:\n",
    "    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()[:16]\n",
    "\n",
    "def fuzzy_choice(text: str, choices: List[str], cutoff: float = 0.86):\n",
    "    if not choices: return None\n",
    "    best = None; best_score = 0.0\n",
    "    for c in choices:\n",
    "        r = difflib.SequenceMatcher(None, text, c).ratio()\n",
    "        if r > best_score:\n",
    "            best, best_score = c, r\n",
    "    return best if best_score >= cutoff else None\n",
    "\n",
    "# === Diagnostics: micro logger (safe to live in Utilities cell) ===\n",
    "import time, os\n",
    "from contextlib import contextmanager\n",
    "\n",
    "DEBUG_TIMING = True  # flip to False to silence all timing output\n",
    "\n",
    "# psutil is optional; we'll degrade gracefully if it's not available\n",
    "try:\n",
    "    import psutil\n",
    "    _PROC = psutil.Process(os.getpid())\n",
    "    def _mem_mb():\n",
    "        try:\n",
    "            return _PROC.memory_info().rss / (1024*1024)\n",
    "        except Exception:\n",
    "            return float('nan')\n",
    "except Exception:\n",
    "    def _mem_mb():\n",
    "        return float('nan')\n",
    "\n",
    "@contextmanager\n",
    "def log_time(label: str):\n",
    "    \"\"\"Usage:\n",
    "    with log_time(\"Stage 3: edible-yield\"):\n",
    "        # code here\n",
    "    \"\"\"\n",
    "    if not DEBUG_TIMING:\n",
    "        yield\n",
    "        return\n",
    "    t0 = time.perf_counter()\n",
    "    m0 = _mem_mb()\n",
    "    print(f\"[timing] ▶ {label} ... (RSS {m0:,.1f} MB)\")\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        dt = time.perf_counter() - t0\n",
    "        m1 = _mem_mb()\n",
    "        print(f\"[timing] ✔ {label} in {dt:,.3f}s (ΔRSS {m1 - m0:+.1f} MB)\")\n",
    "\n",
    "# === GPT bridge used by Stage 1 & Stage 2 ===\n",
    "\n",
    "def _call_gpt(messages, model=None, temperature=0):\n",
    "    \"\"\"\n",
    "    messages: [{\"role\":\"system\"/\"user\"/\"assistant\",\"content\": \"...\"}]\n",
    "    Returns plain string content.\n",
    "    Tries to reuse an existing helper if your notebook already defines one.\n",
    "    \"\"\"\n",
    "    # 1) Compat aliases to reuse your existing helper if present\n",
    "    for alias in (\"call_gpt\", \"gpt_chat\", \"openai_chat\", \"chat_gpt\", \"gpt_call\"):\n",
    "        if alias in globals() and callable(globals()[alias]):\n",
    "            return globals()[alias](messages, model=model, temperature=temperature)\n",
    "\n",
    "    # 2) Minimal OpenAI Chat fallback\n",
    "    import os\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY not set and no existing GPT helper found.\")\n",
    "\n",
    "    try:\n",
    "        # Newer OpenAI client\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(api_key=api_key)\n",
    "        use_model = model or os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "        resp = client.chat.completions.create(\n",
    "            model=use_model,\n",
    "            messages=messages,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "    except Exception:\n",
    "        # Older openai package interface\n",
    "        import openai\n",
    "        openai.api_key = api_key\n",
    "        use_model = model or os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "        resp = openai.ChatCompletion.create(\n",
    "            model=use_model,\n",
    "            messages=messages,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return resp[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd94149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3: Load & Standardize (Enhanced Version) ===\n",
    "# Purpose: Detect vendor format (type01 / type02 / type03) and normalize data to a common schema.\n",
    "# Inputs:\n",
    "#   - input_filename: raw procurement CSV\n",
    "#   - file_type: explicit ('type01'/'type02'/'type03') or 'auto' for autodetection\n",
    "# Dependencies:\n",
    "#   - Utilities from Cell 2 (safe_read_csv, parse_qty_unit, to_lbs)\n",
    "# Outputs:\n",
    "#   - standardized DataFrame with columns:\n",
    "#       ['product', 'description', 'weight_lbs', 'qty' (if available)] !!!!!\n",
    "# Behavior:\n",
    "#   - Reads raw file using safe_read_csv()\n",
    "#   - If file_type='auto', uses comprehensive detection strategies\n",
    "#   - Dispatches to handle_type01/02/03 to standardize column names and compute weights\n",
    "#   - Returns a DataFrame ready for ingredient extraction (Stage 1).\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "def detect_vendor_fingerprint(df: pd.DataFrame) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Detect specific vendor formats based on unique characteristics\n",
    "    \"\"\"\n",
    "    cols_str = ' '.join(str(c).lower() for c in df.columns)\n",
    "    \n",
    "    # Get sample of first column data for content analysis\n",
    "    first_col_sample = df.iloc[:min(50, len(df)), 0].astype(str).str.lower() if len(df) > 0 else pd.Series()\n",
    "    first_col_text = ' '.join(first_col_sample.tolist())\n",
    "    \n",
    "    vendor_signatures = {\n",
    "        \"type01\": [\n",
    "            lambda df: 'quantity (base unit)' in cols_str,\n",
    "            lambda df: 'qty (base unit)' in cols_str,\n",
    "            lambda df: bool(re.search(r'quantity.*base.*unit', cols_str))\n",
    "        ],\n",
    "        \n",
    "        \"type02\": [\n",
    "            lambda df: 'net wght shipped' in cols_str,\n",
    "            lambda df: 'total weight' in cols_str and 'item #' in cols_str,\n",
    "            lambda df: 'qty shipped' in cols_str and 'total weight' in cols_str,\n",
    "            lambda df: 'item description' in cols_str and not 'qty sold' in cols_str\n",
    "        ],\n",
    "        \n",
    "        \"type03\": [\n",
    "            lambda df: 'qty sold cw' in cols_str,\n",
    "            lambda df: 'stock unit' in cols_str,\n",
    "            lambda df: 'item description' in cols_str and 'stock unit' in cols_str,\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for file_type, checks in vendor_signatures.items():\n",
    "        matches = sum(1 for check in checks if check(df))\n",
    "        if matches >= 2:  # Need at least 2 matching signatures\n",
    "            logging.info(f\"Vendor fingerprint detected: {file_type} ({matches} signatures matched)\")\n",
    "            return file_type\n",
    "    \n",
    "    return None\n",
    "\n",
    "def statistical_type_detection(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Use statistical analysis to detect type when column names are ambiguous\n",
    "    \"\"\"\n",
    "    if len(df) == 0:\n",
    "        logging.warning(\"Empty dataframe, defaulting to type01\")\n",
    "        return \"type01\"\n",
    "    \n",
    "    # Analyze first non-header column (usually product/description)\n",
    "    first_col = df.iloc[:, 0].dropna().astype(str)\n",
    "    if len(first_col) == 0:\n",
    "        logging.warning(\"No data in first column, defaulting to type01\")\n",
    "        return \"type01\"\n",
    "    \n",
    "    # Count different pattern occurrences\n",
    "    scores = {\"type01\": 0, \"type02\": 0, \"type03\": 0}\n",
    "    \n",
    "    # Check for type01 patterns (separate qty/unit values like \"80.000 OZ\")\n",
    "    qty_unit_separate = first_col.str.match(r'^\\d+\\.?\\d*\\s+[A-Z]{2,}$', case=False)\n",
    "    scores[\"type01\"] += qty_unit_separate.sum() * 3\n",
    "    \n",
    "    # Also check in second column if exists\n",
    "    if len(df.columns) > 1:\n",
    "        second_col = df.iloc[:, 1].dropna().astype(str)\n",
    "        qty_unit_in_second = second_col.str.match(r'^\\d+\\.?\\d*\\s+[A-Z]{2,}$', case=False)\n",
    "        scores[\"type01\"] += qty_unit_in_second.sum() * 3\n",
    "    \n",
    "    # Check for type02 patterns (clean product names with separate weight column)\n",
    "    clean_products = first_col.str.match(r'^[A-Za-z][A-Za-z\\s\\-\\'\\\"&,\\.]+$')\n",
    "    has_weight_col = any('weight' in str(c).lower() or 'wght' in str(c).lower() for c in df.columns)\n",
    "    has_numeric_col = False\n",
    "    \n",
    "    if len(df.columns) > 1:\n",
    "        for col in df.columns[1:]:\n",
    "            sample = df[col].dropna().head(10)\n",
    "            numeric_ratio = pd.to_numeric(sample, errors='coerce').notna().sum() / max(len(sample), 1)\n",
    "            if numeric_ratio > 0.7:\n",
    "                has_numeric_col = True\n",
    "                break\n",
    "    \n",
    "    if has_weight_col and has_numeric_col:\n",
    "        scores[\"type02\"] += clean_products.sum() * 2\n",
    "        scores[\"type02\"] += 5  # Bonus for having weight column\n",
    "    \n",
    "    # Check for type03 patterns (embedded weights in product description)\n",
    "    # Pattern examples: \"CHEDDAR CLOTHBOUND AGED CABOT 30 LB MC\", \"BLUE BAYLEY HAZEN BLUE 7 LB JASPER HILL\"\n",
    "    embedded_patterns = [\n",
    "        r'\\d+\\s*LB\\s+',  # \"30 LB MC\"\n",
    "        r'\\d+X\\d+\\s*(?:OZ|LB)',  # \"12X12 OZ\"\n",
    "        r'\\d+/\\d+\\s*(?:LB|OZ|GAL)',  # \"6/2.5 LB\"\n",
    "        r'\\d+\\s*(?:LB|OZ|GAL|QT|PT)\\s+[A-Z]',  # \"5 LB CABOT\"\n",
    "        r'[A-Z]+\\s+\\d+\\s*(?:LB|OZ)\\s+[A-Z]'  # \"TUB 5 LB CABOT\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in embedded_patterns:\n",
    "        matches = first_col.str.contains(pattern, case=False, regex=True)\n",
    "        scores[\"type03\"] += matches.sum() * 2\n",
    "    \n",
    "    # Look for case/carton patterns common in type03\n",
    "    case_patterns = first_col.str.contains(r'\\d+/\\d+|\\d+X\\d+|\\d+\\s*(?:CS|CTN|CASE|EACH|EA)', case=False)\n",
    "    scores[\"type03\"] += case_patterns.sum()\n",
    "    \n",
    "    # Additional type03 indicators: mixed units in product names\n",
    "    unit_variety = first_col.str.contains(r'\\b(?:LB|OZ|GAL|EACH|EA|CTN|CS)\\b', case=False)\n",
    "    if unit_variety.sum() > len(first_col) * 0.3:\n",
    "        scores[\"type03\"] += 3\n",
    "    \n",
    "    best_type = max(scores, key=scores.get)\n",
    "    max_score = scores[best_type]\n",
    "    confidence = max_score / max(len(first_col), 1)\n",
    "    \n",
    "    # Log all scores for debugging\n",
    "    logging.debug(f\"Type detection scores: {scores}\")\n",
    "    \n",
    "    if confidence < 0.2 and max_score < 3:\n",
    "        logging.warning(f\"Low confidence detection: {best_type} with score {max_score} (confidence: {confidence:.2f})\")\n",
    "    else:\n",
    "        logging.info(f\"Statistical detection: {best_type} with score {max_score} (confidence: {confidence:.2f})\")\n",
    "    \n",
    "    return best_type\n",
    "\n",
    "def validate_type_detection(df: pd.DataFrame, file_type: str) -> bool:\n",
    "    \"\"\"\n",
    "    Try parsing a sample with the detected type to validate\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use a small sample to test\n",
    "        sample_df = df.head(min(10, len(df))).copy()\n",
    "        \n",
    "        if file_type == \"type01\":\n",
    "            result = handle_type01(sample_df)\n",
    "        elif file_type == \"type02\":\n",
    "            result = handle_type02(sample_df)\n",
    "        elif file_type == \"type03\":\n",
    "            result = handle_type03(sample_df)\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "        # Check if we got reasonable results\n",
    "        if 'weight_lbs' not in result.columns:\n",
    "            return False\n",
    "            \n",
    "        valid_weights = result['weight_lbs'].notna().sum()\n",
    "        valid_ratio = valid_weights / max(len(result), 1)\n",
    "        \n",
    "        # Type03 might have lower valid ratio due to complex parsing\n",
    "        threshold = 0.2 if file_type == \"type03\" else 0.3\n",
    "        \n",
    "        return valid_ratio >= threshold\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.debug(f\"Validation failed for {file_type}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def autodetect_type_improved(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Enhanced autodetection with multiple pattern matching strategies\n",
    "    \"\"\"\n",
    "    if len(df.columns) == 0:\n",
    "        logging.error(\"Auto-detect: Input file has no columns\")\n",
    "        raise ValueError(\"Auto-detect failed: CSV has no columns\")\n",
    "    \n",
    "    # Normalize column names for comparison\n",
    "    cols_lower = [str(c).lower().strip() for c in df.columns]\n",
    "    cols_map = {str(c).lower().strip(): c for c in df.columns}\n",
    "    \n",
    "    # === Type01 Detection ===\n",
    "    type01_col_patterns = [\n",
    "        r'quantity.*base.*unit',\n",
    "        r'qty.*base',\n",
    "        r'product.*weight'\n",
    "    ]\n",
    "    \n",
    "    for pattern in type01_col_patterns:\n",
    "        if any(re.search(pattern, col) for col in cols_lower):\n",
    "            # Additional validation: check if values contain quantity+unit\n",
    "            target_col = None\n",
    "            for col in cols_lower:\n",
    "                if re.search(pattern, col):\n",
    "                    target_col = cols_map[col]\n",
    "                    break\n",
    "            \n",
    "            if target_col and target_col in df.columns:\n",
    "                sample = df[target_col].dropna().head(20).astype(str)\n",
    "                if len(sample) > 0:\n",
    "                    # Check for patterns like \"80.000 OZ\", \"4.400 LB\", \"84.000 lb\"\n",
    "                    qty_unit_pattern = r'^\\d+\\.?\\d*\\s+[A-Za-z]{1,10}$'\n",
    "                    matches = sample.str.match(qty_unit_pattern, case=False)\n",
    "                    if matches.sum() > len(sample) * 0.3:\n",
    "                        logging.info(\"Auto-detect: Type01 - found quantity+unit combined format in column data\")\n",
    "                        return \"type01\"\n",
    "    \n",
    "    # === Type02 Detection ===\n",
    "    has_weight_col = any('weight' in col or 'wght' in col for col in cols_lower)\n",
    "    has_qty_col = any('qty' in col or 'quantity' in col for col in cols_lower)\n",
    "    has_unit_col = any('unit' in col or 'uom' in col for col in cols_lower)\n",
    "    \n",
    "    # Type02 typically has weight column with numeric values and separate qty\n",
    "    if has_weight_col:\n",
    "        weight_cols = [cols_map[c] for c in cols_lower if 'weight' in c or 'wght' in c]\n",
    "        if weight_cols and weight_cols[0] in df.columns:\n",
    "            sample_weights = df[weight_cols[0]].dropna().head(20)\n",
    "            if len(sample_weights) > 0:\n",
    "                numeric_ratio = pd.to_numeric(sample_weights, errors='coerce').notna().sum() / len(sample_weights)\n",
    "                if numeric_ratio > 0.7:\n",
    "                    # Additional check: if we have qty column or no unit column, likely type02\n",
    "                    if has_qty_col or not has_unit_col:\n",
    "                        logging.info(\"Auto-detect: Type02 - separate numeric weight column detected\")\n",
    "                        return \"type02\"\n",
    "    \n",
    "    # === Type03 Detection ===\n",
    "    # Look for product/description columns\n",
    "    product_cols = []\n",
    "    for priority_term in ['product', 'description', 'item']:\n",
    "        for col in cols_lower:\n",
    "            if priority_term in col and cols_map[col] not in product_cols:\n",
    "                product_cols.append(cols_map[col])\n",
    "                break\n",
    "    \n",
    "    # If no product column found, use first column\n",
    "    if not product_cols and len(df.columns) > 0:\n",
    "        product_cols = [df.columns[0]]\n",
    "    \n",
    "    if product_cols and product_cols[0] in df.columns:\n",
    "        sample_products = df[product_cols[0]].dropna().head(30).astype(str)\n",
    "        \n",
    "        if len(sample_products) > 0:\n",
    "            # Look for embedded weight patterns characteristic of type03\n",
    "            embedded_patterns = [\n",
    "                r'\\d+\\s*LB\\b',  # \"30 LB\"\n",
    "                r'\\d+X\\d+\\s*(?:OZ|LB)',  # \"12X12 OZ\"\n",
    "                r'\\d+/\\d+\\s*(?:LB|OZ|GAL)',  # \"6/2.5 LB\"\n",
    "                r'\\b\\d+\\s*(?:LB|OZ|GAL|QT|PT|EA|EACH|CTN|CS)\\b'  # Various units embedded\n",
    "            ]\n",
    "            \n",
    "            pattern_matches = 0\n",
    "            for pattern in embedded_patterns:\n",
    "                pattern_matches += sample_products.str.contains(pattern, case=False, regex=True).sum()\n",
    "            \n",
    "            match_ratio = pattern_matches / len(sample_products)\n",
    "            if match_ratio > 0.4:\n",
    "                logging.info(f\"Auto-detect: Type03 - weight embedded in product description (match ratio: {match_ratio:.2f})\")\n",
    "                return \"type03\"\n",
    "    \n",
    "    # If no clear match from column analysis, use statistical detection\n",
    "    return statistical_type_detection(df)\n",
    "\n",
    "def autodetect_type(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Comprehensive type detection with multiple strategies and validation\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    \n",
    "    # Strategy 1: Check for explicit type markers in first column\n",
    "    if len(df) > 0 and len(df.columns) > 0:\n",
    "        first_col = df.iloc[:, 0].astype(str).str.lower()\n",
    "        for type_name in [\"type01\", \"type02\", \"type03\"]:\n",
    "            if any(type_name in str(val) for val in first_col if pd.notna(val)):\n",
    "                logging.info(f\"Explicit type marker found: {type_name}\")\n",
    "                return type_name\n",
    "    \n",
    "    # Strategy 2: Vendor fingerprinting\n",
    "    vendor_type = detect_vendor_fingerprint(df)\n",
    "    if vendor_type:\n",
    "        # Validate the vendor detection\n",
    "        if validate_type_detection(df, vendor_type):\n",
    "            return vendor_type\n",
    "        else:\n",
    "            logging.warning(f\"Vendor fingerprint {vendor_type} failed validation, continuing detection\")\n",
    "    \n",
    "    # Strategy 3: Enhanced pattern matching\n",
    "    detected_type = autodetect_type_improved(df)\n",
    "    \n",
    "    # Strategy 4: Validation through sample parsing\n",
    "    if validate_type_detection(df, detected_type):\n",
    "        logging.info(f\"Validated type detection: {detected_type}\")\n",
    "        return detected_type\n",
    "    \n",
    "    # If validation fails, try alternative types\n",
    "    logging.warning(f\"Primary detection {detected_type} failed validation, trying alternatives\")\n",
    "    \n",
    "    for alt_type in [\"type01\", \"type02\", \"type03\"]:\n",
    "        if alt_type != detected_type and validate_type_detection(df, alt_type):\n",
    "            logging.info(f\"Alternative type {alt_type} passed validation\")\n",
    "            return alt_type\n",
    "    \n",
    "    # Default to statistical detection if all else fails\n",
    "    final_type = statistical_type_detection(df)\n",
    "    logging.info(f\"Using statistical detection fallback: {final_type}\")\n",
    "    return final_type\n",
    "\n",
    "def handle_type01(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Handle type01 format: quantity and units in combined column\"\"\"\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    \n",
    "    # Ensure we have a product column\n",
    "    if \"product\" not in df.columns:\n",
    "        df[\"product\"] = df.iloc[:, 0].astype(str)\n",
    "    \n",
    "    # Look for the quantity/weight column\n",
    "    if \"product_weight\" in cols:\n",
    "        pw = cols[\"product_weight\"]\n",
    "    elif any(\"quantity\" in c.lower() and \"base\" in c.lower() for c in df.columns):\n",
    "        # Find the quantity base unit column\n",
    "        pw = next(c for c in df.columns if \"quantity\" in c.lower() and \"base\" in c.lower())\n",
    "    else:\n",
    "        # Fallback: look for a column with quantity+unit patterns\n",
    "        pw = None\n",
    "        for col in df.columns[1:]:  # Skip first column (likely product name)\n",
    "            sample = df[col].dropna().head(10).astype(str)\n",
    "            if sample.str.match(r'^\\d+\\.?\\d*\\s+[A-Za-z]+$', case=False).sum() > len(sample) * 0.3:\n",
    "                pw = col\n",
    "                break\n",
    "    \n",
    "    if pw and pw in df.columns:\n",
    "        # Split the quantity and unit\n",
    "        parts = df[pw].astype(str).str.strip().str.split(r'\\s+', n=1, expand=True)\n",
    "        if parts.shape[1] == 2:\n",
    "            df[\"qty\"] = pd.to_numeric(parts[0].str.replace(\",\", \"\", regex=False), errors=\"coerce\")\n",
    "            df[\"uom\"] = parts[1]\n",
    "            df[\"weight_lbs\"] = [to_lbs(q, u) if pd.notna(q) and pd.notna(u) else np.nan \n",
    "                               for q, u in zip(df[\"qty\"], df[\"uom\"])]\n",
    "        else:\n",
    "            df[\"qty\"] = pd.to_numeric(parts[0], errors=\"coerce\")\n",
    "            df[\"weight_lbs\"] = np.nan\n",
    "    else:\n",
    "        # Extract from product column as fallback\n",
    "        q, u = zip(*df[\"product\"].map(parse_qty_unit))\n",
    "        df[\"qty\"], df[\"uom\"] = q, u\n",
    "        df[\"weight_lbs\"] = [to_lbs(q, u) if q and u else np.nan \n",
    "                           for q, u in zip(df[\"qty\"], df[\"uom\"])]\n",
    "    \n",
    "    df[\"description\"] = df[\"product\"]\n",
    "    base = [\"product\", \"description\", \"weight_lbs\"]\n",
    "    if \"qty\" in df.columns:\n",
    "        base.append(\"qty\")\n",
    "    return df[base]\n",
    "\n",
    "def handle_type02(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Handle type02 format: separate weight column in pounds\"\"\"\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    \n",
    "    # Find product column\n",
    "    prod_col = cols.get(\"product\") or cols.get(\"item description\") or cols.get(\"description\") or df.columns[0]\n",
    "    df[\"product\"] = df[prod_col].astype(str)\n",
    "    \n",
    "    # Find weight column\n",
    "    prod_wt_col = (cols.get(\"product_weight\") or cols.get(\"total weight\") or \n",
    "                   cols.get(\"weight\") or cols.get(\"total_weight\") or \n",
    "                   cols.get(\"net wght shipped\"))\n",
    "    \n",
    "    # Find unit and quantity columns\n",
    "    unit_col = (cols.get(\"unit\") or cols.get(\"uom\") or \n",
    "                cols.get(\"unit_of_measure\") or cols.get(\"unit_of_measurement\"))\n",
    "    qty_col = (cols.get(\"qty\") or cols.get(\"quantity\") or \n",
    "               cols.get(\"qty shipped\") or cols.get(\"qty_shipped\"))\n",
    "    \n",
    "    if qty_col and qty_col in df.columns:\n",
    "        df[\"qty\"] = pd.to_numeric(df[qty_col], errors=\"coerce\")\n",
    "    \n",
    "    # Calculate weight in pounds\n",
    "    weight_lbs = None\n",
    "    if prod_wt_col and prod_wt_col in df.columns:\n",
    "        wt_numeric = pd.to_numeric(df[prod_wt_col], errors=\"coerce\")\n",
    "        \n",
    "        if unit_col and unit_col in df.columns and df[unit_col].notna().any():\n",
    "            # If we have units, convert accordingly\n",
    "            unit_series = df[unit_col].astype(str).str.upper()\n",
    "            factors = unit_series.map(lambda u: UOM_TO_LBS.get(u, np.nan))\n",
    "            weight_lbs = wt_numeric * factors\n",
    "            \n",
    "            # If already in pounds, use as-is\n",
    "            mask_lbs = unit_series.isin([\"LB\", \"LBS\", \"POUND\", \"POUNDS\"])\n",
    "            weight_lbs = weight_lbs.where(~mask_lbs, wt_numeric)\n",
    "        else:\n",
    "            # Assume weights are in pounds (common for type02)\n",
    "            weight_lbs = wt_numeric\n",
    "    elif qty_col and unit_col and qty_col in df.columns and unit_col in df.columns:\n",
    "        # Calculate from qty and unit\n",
    "        qty = pd.to_numeric(df[qty_col], errors=\"coerce\")\n",
    "        uom = df[unit_col].astype(str)\n",
    "        weight_lbs = pd.Series([to_lbs(q, u) if (pd.notna(q) and pd.notna(u)) else np.nan \n",
    "                               for q, u in zip(qty, uom)], index=df.index)\n",
    "    else:\n",
    "        # Try to extract from product description as last resort\n",
    "        q, u = zip(*df[\"product\"].map(parse_qty_unit))\n",
    "        weight_lbs = pd.Series([to_lbs(qi, ui) if qi and ui else np.nan \n",
    "                               for qi, ui in zip(q, u)], index=df.index)\n",
    "    \n",
    "    df[\"weight_lbs\"] = weight_lbs\n",
    "    df[\"description\"] = df[\"product\"]\n",
    "    \n",
    "    base = [\"product\", \"description\", \"weight_lbs\"]\n",
    "    if \"qty\" in df.columns:\n",
    "        base.append(\"qty\")\n",
    "    return df[base]\n",
    "\n",
    "def handle_type03(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Handle type03 format: weight info embedded in product description\"\"\"\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    \n",
    "    # Find product column\n",
    "    prod_col = (cols.get(\"product\") or cols.get(\"item description\") or \n",
    "                cols.get(\"description\") or df.columns[0])\n",
    "    df[\"product\"] = df[prod_col].astype(str)\n",
    "    \n",
    "    # Look for a quantity sold column (common in Baldor format)\n",
    "    qty_col = cols.get(\"qty sold cw\") or cols.get(\"qty\") or cols.get(\"quantity\")\n",
    "    if qty_col and qty_col in df.columns:\n",
    "        df[\"qty_sold\"] = pd.to_numeric(df[qty_col], errors=\"coerce\")\n",
    "    \n",
    "    # Extract weight from product description\n",
    "    q, u = zip(*df[\"product\"].map(parse_qty_unit))\n",
    "    df[\"qty\"] = pd.to_numeric(pd.Series(q, index=df.index), errors=\"coerce\")\n",
    "    \n",
    "    # Calculate weight in pounds\n",
    "    maybe = [to_lbs(qi, ui) if qi and ui else np.nan for qi, ui in zip(q, u)]\n",
    "    df[\"weight_lbs\"] = pd.Series(maybe, index=df.index)\n",
    "    \n",
    "    # If we have qty_sold and a unit column, we might be able to calculate total weight\n",
    "    if \"qty_sold\" in df.columns:\n",
    "        unit_col = cols.get(\"stock unit\") or cols.get(\"unit\")\n",
    "        if unit_col and unit_col in df.columns:\n",
    "            # For items sold by weight units (LB, OZ, etc.), multiply qty_sold by unit weight\n",
    "            unit_series = df[unit_col].astype(str).str.upper()\n",
    "            \n",
    "            # Create a mask for weight-based units\n",
    "            weight_units = [\"LB\", \"LBS\", \"OZ\", \"KG\", \"G\", \"POUND\", \"POUNDS\", \"OUNCE\", \"OUNCES\"]\n",
    "            is_weight_unit = unit_series.isin(weight_units)\n",
    "            \n",
    "            # For weight units, calculate total weight\n",
    "            for idx in df[is_weight_unit].index:\n",
    "                unit = unit_series[idx]\n",
    "                qty_sold = df.loc[idx, \"qty_sold\"]\n",
    "                if pd.notna(qty_sold):\n",
    "                    total_weight = to_lbs(qty_sold, unit)\n",
    "                    if pd.notna(total_weight):\n",
    "                        df.loc[idx, \"weight_lbs\"] = total_weight\n",
    "        \n",
    "        # Use qty_sold as qty if we don't have qty from parsing\n",
    "        if df[\"qty\"].isna().all() and df[\"qty_sold\"].notna().any():\n",
    "            df[\"qty\"] = df[\"qty_sold\"]\n",
    "    \n",
    "    df[\"description\"] = df[\"product\"]\n",
    "    base = [\"product\", \"description\", \"weight_lbs\", \"qty\"]\n",
    "    return df[base]\n",
    "\n",
    "def load_and_standardize(path: str, file_type: str = \"auto\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Main function to load and standardize procurement data\n",
    "    \n",
    "    Args:\n",
    "        path: Path to the CSV file\n",
    "        file_type: One of 'auto', 'type01', 'type02', 'type03'\n",
    "    \n",
    "    Returns:\n",
    "        Standardized DataFrame with columns: product, description, weight_lbs, qty (optional)\n",
    "    \"\"\"\n",
    "    # Load the raw data\n",
    "    raw = safe_read_csv(path)\n",
    "    \n",
    "    # Determine the file type\n",
    "    ft = file_type\n",
    "    if ft == \"auto\":\n",
    "        ft = autodetect_type(raw)\n",
    "        logging.info(f\"Auto-detected file_type: {ft}\")\n",
    "    elif ft not in [\"type01\", \"type02\", \"type03\"]:\n",
    "        raise ValueError(\"file_type must be one of {'auto','type01','type02','type03'}\")\n",
    "    \n",
    "    # Process according to detected/specified type\n",
    "    if ft == \"type01\":\n",
    "        std = handle_type01(raw)\n",
    "    elif ft == \"type02\":\n",
    "        std = handle_type02(raw)\n",
    "    elif ft == \"type03\":\n",
    "        std = handle_type03(raw)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected file_type: {ft}\")\n",
    "    \n",
    "    # Log summary statistics\n",
    "    total_rows = len(std)\n",
    "    valid_weights = std[\"weight_lbs\"].notna().sum()\n",
    "    logging.info(f\"Standardization complete: {valid_weights}/{total_rows} rows with valid weights ({valid_weights/total_rows*100:.1f}%)\")\n",
    "    \n",
    "    if valid_weights / total_rows < 0.3:\n",
    "        logging.warning(\"Low weight extraction rate. Consider checking the file format or type detection.\")\n",
    "    \n",
    "    return std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92848327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4 — AI Ingredient Extraction ===\n",
    "# Purpose: Define GPT-based function(s) that infer ingredient lists and estimated proportions for each product.\n",
    "# Includes: Guard lists (protected items, vegan/meatless handling, fish terms) and exact category helpers.\n",
    "# Inputs:\n",
    "#   - product text string(s)\n",
    "#   - optional explicit_ingredients (for compatibility with non-AI modes)\n",
    "# Dependencies:\n",
    "#   - Utilities from Cell 2 (_norm, safe_json, etc.)\n",
    "#   - Config flags from Cell 1 (PROTECTED_ITEMS, VEGAN_GUARD)\n",
    "# Outputs:\n",
    "#   - List[Dict[str, float]] per product, each containing {\"ingredient\": <name>, \"percent\": <float>}\n",
    "# Non-decomposition rules & exact category resolvers used by Stages 1 & 2.\n",
    "# Behavior:\n",
    "#   - Calls GPT via ai_extract_ingredients() with structured prompts.\n",
    "#   - Applies post-processing:\n",
    "#       • normalize capitalization and spacing\n",
    "#       • protect compound food names (e.g., “ice cream”, “almond milk”)\n",
    "#       • enforce vegan/meatless exclusion filters.\n",
    "\n",
    "import re\n",
    "from typing import List, Dict, Optional\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def _norm(x: str) -> str:\n",
    "    \"\"\"Use the notebook's normalizer so everything stays consistent.\"\"\"\n",
    "    return norm_text(x)\n",
    "\n",
    "def _canon_key(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Canonical key for STRICT equality:\n",
    "      - lowercase\n",
    "      - replace any non-alphanumeric with a single space ((),/,&,commas, hyphens, etc.)\n",
    "      - collapse whitespace, strip\n",
    "    NOTE: This is still equality-based (no substring matching).\n",
    "    \"\"\"\n",
    "    s = str(s or \"\").lower()\n",
    "    s = re.sub(r\"[^0-9a-z]+\", \" \", s)     # punctuation/paren → space\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# ------------------ Load factors categories (source of truth) ------------------\n",
    "try:\n",
    "    _fac_df = safe_read_csv(factors_filename)  # prefer your safer loader\n",
    "except Exception:\n",
    "    _fac_df = pd.read_csv(factors_filename)\n",
    "\n",
    "if \"category\" in _fac_df.columns:\n",
    "    _fac_series = _fac_df[\"category\"].astype(str)\n",
    "else:\n",
    "    _fac_series = _fac_df.iloc[:, 0].astype(str)\n",
    "\n",
    "ALLOWED_CATEGORIES: List[str] = [c.strip() for c in _fac_series.tolist() if str(c).strip()]\n",
    "\n",
    "# Canonical maps keyed by strict canonical equality (no substrings)\n",
    "CANON_CAT_BY_CANON: Dict[str, str] = {}\n",
    "EXACT_ALLOWED_CATEGORIES: set[str] = set()\n",
    "for cat in ALLOWED_CATEGORIES:\n",
    "    k = _canon_key(cat)\n",
    "    if not k or k == \"unknown\":\n",
    "        continue\n",
    "    EXACT_ALLOWED_CATEGORIES.add(k)\n",
    "    # Preserve original label as the canonical output\n",
    "    CANON_CAT_BY_CANON.setdefault(k, cat)\n",
    "\n",
    "# ------------------ Category map (from your foodcategories.json) ------------------\n",
    "def load_category_map(json_path: Path) -> Dict[str, str]:\n",
    "    \"\"\"Return normalized-name -> category map loaded earlier (FOODCATS).\"\"\"\n",
    "    return FOODCATS\n",
    "\n",
    "CATEGORY_MAP = load_category_map(categories_filename)\n",
    "\n",
    "# ------------------ Protections used by later cells ------------------\n",
    "DO_NOT_DECOMPOSE = set(map(_norm, [\n",
    "    # Core dairy intact items:\n",
    "    \"milk\",\"dairy milk\",\"skim milk\",\"concentrated milk\",\"buttermilk\",\n",
    "    \"cheese\",\"yogurt\",\"low fat yogurt\",\"cream\",\"sour cream\",\"butter\",\"ghee\",\n",
    "    \"ice cream\",\"whey powder\",\"milk powder\",\n",
    "    # Plant milks intact as single items:\n",
    "    \"almond milk\",\"oat milk\",\"soy milk\",\"rice milk\",\"coconut milk\",\"pea milk\",\n",
    "]))\n",
    "\n",
    "# Minimal fish list so downstream checks won’t fail; include milkfish explicitly.\n",
    "FISH_SPECIES = set(map(_norm, [\n",
    "    \"salmon\",\"trout\",\"tilapia\",\"catfish\",\"bass\",\"cod\",\"pollock\",\"haddock\",\"mahi\",\n",
    "    \"tuna\",\"sardine\",\"anchovy\",\"halibut\",\"carp\",\"redfish\",\"snapper\",\"milkfish\"\n",
    "]))\n",
    "\n",
    "ANIMAL_INGREDIENTS = set(map(_norm, [\n",
    "    \"milk\",\"dairy milk\",\"cheese\",\"butter\",\"cream\",\"yogurt\",\"whey\",\"casein\",\"ghee\",\"buttermilk\",\"lactose\",\n",
    "    \"egg\",\"egg whites\",\"egg yolk\",\"honey\",\"gelatin\", \"dairy cheese\",\n",
    "    \"beef\",\"pork\",\"chicken\",\"turkey\",\"lamb\",\"bacon\",\"sausage\",\n",
    "    \"fish\",\"salmon\",\"tuna\",\"shrimp\",\"tilapia\",\"trout\",\"catfish\",\"bass\",\"redfish\",\"milkfish\"\n",
    "]))\n",
    "\n",
    "# ------------------ Prioritized list = EXACT categories only (for prompts, if used) ------------------\n",
    "PRIORITIZED_INGREDIENTS: List[str] = sorted({CANON_CAT_BY_CANON[k] for k in EXACT_ALLOWED_CATEGORIES})\n",
    "\n",
    "# ------------------ Exact-only resolver (canonical equality, no substrings) ------------------\n",
    "def resolve_exact_category(product_text: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    STRICT equality after canonicalization:\n",
    "      1) If canon(product) equals canon(category) from factors.csv → return that category (canonical label).\n",
    "      2) Else, if product matches a key in CATEGORY_MAP and that mapped value is a valid factors category,\n",
    "         accept the mapped category (also via canonical equality).\n",
    "      3) Else, return None.\n",
    "\n",
    "    Notes:\n",
    "      - No token splitting, no partial/substring/phrase matching.\n",
    "      - 'milkfish' will NOT equal 'milk' in canonical form.\n",
    "      - 'barley (beer)' equals 'barley beer' canonically on both sides, so it matches cleanly.\n",
    "      - Multi-word categories like 'low fat yogurt' remain distinct from 'yogurt'.\n",
    "    \"\"\"\n",
    "    name_can = _canon_key(product_text)\n",
    "    if not name_can:\n",
    "        return None\n",
    "\n",
    "    # (1) Direct exact to factors category\n",
    "    if name_can in CANON_CAT_BY_CANON:\n",
    "        return CANON_CAT_BY_CANON[name_can]\n",
    "\n",
    "    # (2) Exact to a key in CATEGORY_MAP, but only accept if mapped value is in factors\n",
    "    #     (We canonicalize the mapped value and check against CANON_CAT_BY_CANON.)\n",
    "    k_norm = _norm(product_text)\n",
    "    if k_norm in CATEGORY_MAP:\n",
    "        mapped = CATEGORY_MAP[k_norm]\n",
    "        mapped_can = _canon_key(mapped)\n",
    "        if mapped_can in CANON_CAT_BY_CANON:\n",
    "            return CANON_CAT_BY_CANON[mapped_can]\n",
    "\n",
    "    return None\n",
    "\n",
    "logging.info(\n",
    "    \"[Cell 4] Exact-only canonical mode: %d categories; PRIORITIZED_INGREDIENTS=%d; DO_NOT_DECOMPOSE=%d\",\n",
    "    len(EXACT_ALLOWED_CATEGORIES), len(PRIORITIZED_INGREDIENTS), len(DO_NOT_DECOMPOSE)\n",
    ")\n",
    "\n",
    "# --- Adapter for legacy calls expecting `categorize_name_from_map` ---\n",
    "def categorize_name_from_map(name: str) -> Optional[str]:\n",
    "# Use the exact-only canonical resolver\n",
    "    return resolve_exact_category(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a23fc06-4eca-4714-96e0-088bf73a96d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5 — Stage 1 (type03) Runner ===\n",
    "# Purpose: Execute AI ingredient extraction for type03 vendor files.\n",
    "# Inputs:\n",
    "#   - standardized DataFrame from Cell 3 (type03 schema)\n",
    "#   - external item_weights.json (optional overrides)\n",
    "#   - ai_extract_ingredients() from Cell 4\n",
    "# Outputs:\n",
    "#   - ingredients_{BASE}.csv (one row per product–ingredient pair, with estimated weights)\n",
    "# Behavior:\n",
    "#   - Merges item_weights overrides if present.\n",
    "#   - Applies GPT extraction per product description.\n",
    "#   - Normalizes percentages to sum ≈ 100 %.\n",
    "#   - Logs progress and total ingredient count.\n",
    "# Overwrites existing ingredients_{BASE}.csv if START_MODE='full'; skips otherwise.\n",
    "\n",
    "import os, json, re, concurrent.futures as cf\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# === Guard flag ===\n",
    "_run_cell5 = False\n",
    "try:\n",
    "    _run_cell5 = str(file_type).lower() == \"type03\"\n",
    "except NameError:\n",
    "    _run_cell5 = False\n",
    "\n",
    "# --- Logger ---\n",
    "try:\n",
    "    log_time\n",
    "except NameError:\n",
    "    from contextlib import contextmanager\n",
    "    import time\n",
    "    @contextmanager\n",
    "    def log_time(label: str):\n",
    "        t0 = time.perf_counter()\n",
    "        print(f\"[timing] ▶ {label} ...\")\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            dt = time.perf_counter() - t0\n",
    "            print(f\"[timing] ✓ {label} in {dt:,.3f}s\")\n",
    "\n",
    "DEBUG_WEIGHT_CALC = True\n",
    "\n",
    "def debug_print(msg: str, row_sheet: Optional[int] = None):\n",
    "    if DEBUG_WEIGHT_CALC:\n",
    "        if row_sheet is not None:\n",
    "            print(f\"[Row {row_sheet}] {msg}\")\n",
    "        else:\n",
    "            print(msg)\n",
    "\n",
    "# ============================================================================\n",
    "# WEIGHT EXTRACTION WITH ALL FIXES (v110b)\n",
    "# ============================================================================\n",
    "\n",
    "ITEM_WEIGHTS = {}\n",
    "\n",
    "def load_item_weights():\n",
    "    global ITEM_WEIGHTS\n",
    "    weights_file = Path(\"item_weights.json\")\n",
    "    if not weights_file.exists():\n",
    "        print(f\"[Warning] {weights_file} not found. Using minimal defaults.\")\n",
    "        ITEM_WEIGHTS = {\n",
    "            r'\\bwatermelons?\\b': 15.0,\n",
    "            r'\\bcantaloupes?\\b': 3.0,\n",
    "            r'\\bhoneydewe?s?\\b': 5.0,\n",
    "            r'\\bpineapples?\\b': 4.0,\n",
    "            r'\\bapples?\\b': 0.33,\n",
    "        }\n",
    "        return\n",
    "    try:\n",
    "        with open(weights_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        for item_name, weight in data.items():\n",
    "            pattern = rf'\\b{item_name}e?s?\\b'\n",
    "            ITEM_WEIGHTS[pattern] = weight\n",
    "        print(f\"[Config] Loaded {len(ITEM_WEIGHTS)} item weights from {weights_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to load {weights_file}: {e}\")\n",
    "        ITEM_WEIGHTS = {r'\\bapples?\\b': 0.33}\n",
    "\n",
    "load_item_weights()\n",
    "\n",
    "WEIGHT_UNITS = {\"LB\": 1.0, \"LBS\": 1.0, \"OZ\": 1.0/16.0, \"KG\": 2.20462262, \"G\": 0.00220462262}\n",
    "CAN_WEIGHTS = {10: 6.5, 5: 3.5, 2: 1.5}\n",
    "BUSHEL_WEIGHTS = {\n",
    "    'lima bean': 30, 'bean': 30, 'beet': 50, 'cabbage': 50, 'carrot': 50,\n",
    "    'cucumber': 59, 'eggplant': 34, 'okra': 30, 'onion': 52, 'parsnip': 50,\n",
    "    'potato': 60, 'rutabaga': 53, 'spinach': 23, 'greens': 23, 'squash': 40,\n",
    "    'sweet potato': 52, 'tomato': 55, 'turnip': 55, 'apple': 48,\n",
    "    'blueberry': 42, 'cantaloupe': 50, 'peach': 50, 'pear': 50, 'plum': 50,\n",
    "    'pepper': 28, 'corn': 56,\n",
    "}\n",
    "\n",
    "def get_bushel_weight(product_text: str) -> float:\n",
    "    text_lower = product_text.lower()\n",
    "    for product_name, weight in BUSHEL_WEIGHTS.items():\n",
    "        if product_name in text_lower:\n",
    "            return weight\n",
    "    return 35\n",
    "\n",
    "def get_liquid_density_per_liter(product_text: str) -> float:\n",
    "    text_lower = product_text.lower()\n",
    "    if re.search(r'\\b(water|tonic)\\b', text_lower):\n",
    "        return 2.20462\n",
    "    elif re.search(r'\\b(juice|cider)\\b', text_lower):\n",
    "        return 2.25\n",
    "    elif re.search(r'\\b(oil)\\b', text_lower):\n",
    "        return 2.03\n",
    "    elif re.search(r'\\b(vinegar|wine)\\b', text_lower):\n",
    "        return 2.20\n",
    "    elif re.search(r'\\b(honey|syrup)\\b', text_lower):\n",
    "        return 3.31\n",
    "    else:\n",
    "        return 2.20462\n",
    "\n",
    "def get_liquid_density(product_text: str) -> float:\n",
    "    text_lower = product_text.lower()\n",
    "    if re.search(r'\\b(cider|juice|lemonade)\\b', text_lower):\n",
    "        return 8.6\n",
    "    elif re.search(r'\\b(olive oil|oil)\\b', text_lower):\n",
    "        return 7.5\n",
    "    elif re.search(r'\\b(vinegar|wine)\\b', text_lower):\n",
    "        return 8.4\n",
    "    elif re.search(r'\\b(honey|syrup)\\b', text_lower):\n",
    "        return 12.0\n",
    "    else:\n",
    "        return 8.34\n",
    "\n",
    "def _parse_fraction(num_str):\n",
    "    if '/' in num_str:\n",
    "        parts = num_str.split('/')\n",
    "        if len(parts) == 2:\n",
    "            try:\n",
    "                return float(parts[0]) / float(parts[1])\n",
    "            except:\n",
    "                return float(num_str.replace('/', ''))\n",
    "    return float(num_str)\n",
    "\n",
    "def extract_weight_from_text(product_text: str, row_sheet: Optional[int] = None):\n",
    "    if not product_text:\n",
    "        return None, \"unknown\"\n",
    "    \n",
    "    product_upper = product_text.upper()\n",
    "    bushel_wt = get_bushel_weight(product_text)\n",
    "    \n",
    "    # BUSHEL PATTERNS (support both \"1-1/9 BU\" and \"BU 1-1/9\" formats)\n",
    "    \n",
    "    # Format: BU 1-1/9 (unit before number)\n",
    "    bushel_match = re.search(r'(?:BU|BSH|BUSH)\\b\\s*(\\d+)-(\\d+)/(\\d+)', product_upper)\n",
    "    if bushel_match:\n",
    "        whole, num, denom = int(bushel_match.group(1)), int(bushel_match.group(2)), int(bushel_match.group(3))\n",
    "        result = (whole + num/denom) * bushel_wt\n",
    "        debug_print(f\"'{product_text}' matched bushel (unit-first mixed) → {result:.4f} lbs\", row_sheet)\n",
    "        return round(result, 4), \"weight_parsed\"\n",
    "    \n",
    "    # Format: 1-1/9 BU (number before unit)\n",
    "    bushel_match = re.search(r'(\\d+)-(\\d+)/(\\d+)\\s*(?:BU|BSH|BUSH)\\b', product_upper)\n",
    "    if bushel_match:\n",
    "        whole, num, denom = int(bushel_match.group(1)), int(bushel_match.group(2)), int(bushel_match.group(3))\n",
    "        result = (whole + num/denom) * bushel_wt\n",
    "        debug_print(f\"'{product_text}' matched bushel (mixed) → {result:.4f} lbs\", row_sheet)\n",
    "        return round(result, 4), \"weight_parsed\"\n",
    "    \n",
    "    bushel_match = re.search(r'(\\d+)\\s+(\\d+)/(\\d+)\\s*(?:BU|BSH|BUSH)\\b', product_upper)\n",
    "    if bushel_match:\n",
    "        whole, num, denom = int(bushel_match.group(1)), int(bushel_match.group(2)), int(bushel_match.group(3))\n",
    "        result = (whole + num/denom) * bushel_wt\n",
    "        debug_print(f\"'{product_text}' matched bushel (space) → {result:.4f} lbs\", row_sheet)\n",
    "        return round(result, 4), \"weight_parsed\"\n",
    "    \n",
    "    # Format: BU 1/2 (unit before fraction)\n",
    "    bushel_match = re.search(r'(?:BU|BSH|BUSH)\\b\\s*(?:1/2|½)', product_upper)\n",
    "    if bushel_match:\n",
    "        result = 0.5 * bushel_wt\n",
    "        debug_print(f\"'{product_text}' matched bushel (unit-first half) → {result:.4f} lbs\", row_sheet)\n",
    "        return round(result, 4), \"weight_parsed\"\n",
    "    \n",
    "    # Format: 1/2 BU (fraction before unit)\n",
    "    bushel_match = re.search(r'(?:1/2|½)\\s*(?:BU|BSH|BUSH)\\b', product_upper)\n",
    "    if bushel_match:\n",
    "        result = 0.5 * bushel_wt\n",
    "        debug_print(f\"'{product_text}' matched bushel (half) → {result:.4f} lbs\", row_sheet)\n",
    "        return round(result, 4), \"weight_parsed\"\n",
    "    \n",
    "    # Format: BU 1/4 (unit before simple fraction)\n",
    "    bushel_match = re.search(r'(?:BU|BSH|BUSH)\\b\\s*(\\d+)/(\\d+)', product_upper)\n",
    "    if bushel_match:\n",
    "        result = (int(bushel_match.group(1))/int(bushel_match.group(2))) * bushel_wt\n",
    "        debug_print(f\"'{product_text}' matched bushel (unit-first fraction) → {result:.4f} lbs\", row_sheet)\n",
    "        return round(result, 4), \"weight_parsed\"\n",
    "    \n",
    "    # Format: 1/4 BU (simple fraction before unit)\n",
    "    bushel_match = re.search(r'(\\d+)/(\\d+)\\s*(?:BU|BSH|BUSH)\\b', product_upper)\n",
    "    if bushel_match:\n",
    "        result = (int(bushel_match.group(1))/int(bushel_match.group(2))) * bushel_wt\n",
    "        debug_print(f\"'{product_text}' matched bushel (fraction) → {result:.4f} lbs\", row_sheet)\n",
    "        return round(result, 4), \"weight_parsed\"\n",
    "    \n",
    "    # Format: BU 2 (unit before whole number)\n",
    "    bushel_match = re.search(r'(?:BU|BSH|BUSH)\\b\\s*(\\d+(?:\\.\\d+)?)', product_upper)\n",
    "    if bushel_match:\n",
    "        result = float(bushel_match.group(1)) * bushel_wt\n",
    "        debug_print(f\"'{product_text}' matched bushel (unit-first whole) → {result:.4f} lbs\", row_sheet)\n",
    "        return round(result, 4), \"weight_parsed\"\n",
    "    \n",
    "    # Format: 2 BU (whole number before unit)\n",
    "    bushel_match = re.search(r'\\b(\\d+(?:\\.\\d+)?)\\s*(?:BU|BSH|BUSH)\\b', product_upper)\n",
    "    if bushel_match:\n",
    "        result = float(bushel_match.group(1)) * bushel_wt\n",
    "        debug_print(f\"'{product_text}' matched bushel (whole) → {result:.4f} lbs\", row_sheet)\n",
    "        return round(result, 4), \"weight_parsed\"\n",
    "\n",
    "    # CASE PATTERNS (CS = case, typical produce case weights)\n",
    "    case_weights = {\n",
    "        'berry': 12, 'berries': 12, 'strawberry': 12, 'strawberries': 12,\n",
    "        'blueberry': 12, 'blueberries': 12, 'raspberry': 12, 'raspberries': 12,\n",
    "        'blackberry': 12, 'blackberries': 12,\n",
    "        'lettuce': 24, 'cabbage': 50, 'broccoli': 20, 'cauliflower': 20,\n",
    "        'pepper': 25, 'peppers': 25, 'tomato': 25, 'tomatoes': 25,\n",
    "        'cucumber': 24, 'cucumbers': 24, 'squash': 20, 'zucchini': 20,\n",
    "        'onion': 50, 'onions': 50, 'carrot': 50, 'carrots': 50,\n",
    "        'celery': 55, 'scallion': 15, 'scallions': 15, 'greens': 20,\n",
    "        'okra': 15, 'watercress': 4,\n",
    "    }\n",
    "    \n",
    "    def get_case_weight(product_text: str) -> float:\n",
    "        text_lower = product_text.lower()\n",
    "        for product_name, weight in case_weights.items():\n",
    "            if product_name in text_lower:\n",
    "                return weight\n",
    "        return 25  # default case weight\n",
    "    \n",
    "    # Match \"1/2 CS\" or \"CS\" patterns\n",
    "    case_match = re.search(r'(?:1/2|½)\\s*CS\\b', product_upper)\n",
    "    if case_match:\n",
    "        result = get_case_weight(product_text) * 0.5\n",
    "        debug_print(f\"'{product_text}' matched half-case → {result:.4f} lbs\", row_sheet)\n",
    "        return round(result, 4), \"weight_parsed\"\n",
    "    \n",
    "    case_match = re.search(r'\\bCS\\b', product_upper)\n",
    "    if case_match:\n",
    "        result = get_case_weight(product_text)\n",
    "        debug_print(f\"'{product_text}' matched full case → {result:.4f} lbs\", row_sheet)\n",
    "        return round(result, 4), \"weight_parsed\"\n",
    "\n",
    "    # WEIGHT RANGES\n",
    "    range_match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*-\\s*(\\d+(?:\\.\\d+)?)\\s+(lb|lbs|oz|kg|g)\\b', product_text, flags=re.I)\n",
    "    if range_match:\n",
    "        midpoint = (float(range_match.group(1)) + float(range_match.group(2))) / 2\n",
    "        unit = range_match.group(3).upper()\n",
    "        if unit in WEIGHT_UNITS:\n",
    "            result = midpoint * WEIGHT_UNITS[unit]\n",
    "            debug_print(f\"'{product_text}' matched weight range → {result:.4f} lbs\", row_sheet)\n",
    "            return result, \"weight_range\"\n",
    "            \n",
    "    # WEIGHT RANGES\n",
    "#    range_match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*-\\s*(\\d+(?:\\.\\d+)?)\\s+(lb|lbs|oz|kg|g)\\b', product_text, flags=re.I)\n",
    "#    if range_match:\n",
    "#        midpoint = (float(range_match.group(1)) + float(range_match.group(2))) / 2\n",
    "#        unit = range_match.group(3).upper()\n",
    "#        if unit in WEIGHT_UNITS:\n",
    "#            return midpoint * WEIGHT_UNITS[unit], \"weight_range\"\n",
    "\n",
    "    # CT + WEIGHT\n",
    "    ct_weight_patterns = [\n",
    "        (r'\\b(\\d+)\\s*CT\\s+(\\d+(?:\\.\\d+)?)\\s*LBS?\\b', lambda m: float(m.group(2))),\n",
    "        (r'\\b(\\d+)\\s*CT\\s+(\\d+(?:\\.\\d+)?)\\s*KG\\b', lambda m: float(m.group(2)) * 2.20462262),\n",
    "        (r'\\b(\\d+)\\s*CT\\s+(\\d+(?:\\.\\d+)?)\\s*OZ\\b', lambda m: float(m.group(2)) / 16),\n",
    "    ]\n",
    "    for pattern, calc_func in ct_weight_patterns:\n",
    "        match = re.search(pattern, product_upper)\n",
    "        if match:\n",
    "            try:\n",
    "                result = calc_func(match)\n",
    "                if result and result > 0:\n",
    "                    debug_print(f\"'{product_text}' matched CT+weight → {result:.4f} lbs\", row_sheet)\n",
    "                    return round(result, 4), \"ct_total_weight\"\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # PATTERNS\n",
    "    LB_TO_KG = 0.45359237\n",
    "    patterns = [\n",
    "        (r'\\b(\\d+(?:\\.\\d+)?)\\s*DOZ\\b', lambda m: float(m.group(1)) * 1.5),\n",
    "        (r'(\\d+)X#(\\d+)\\b', lambda m: int(m.group(1)) * CAN_WEIGHTS.get(int(m.group(2)), 6.5)),\n",
    "        (r'(\\d+)X(\\d*\\.?\\d+)\\s*OZ\\b', lambda m: int(m.group(1)) * float(m.group(2)) / 16),\n",
    "        (r'(\\d+)X(\\d*\\.?\\d+)\\s*LBS?\\b', lambda m: int(m.group(1)) * float(m.group(2))),\n",
    "        (r'(\\d+)X(\\d*\\.?\\d+)\\s*KG\\b', lambda m: int(m.group(1)) * float(m.group(2)) / LB_TO_KG),\n",
    "        (r'(\\d+)X(\\d*\\.?\\d+)\\s*GR?\\b', lambda m: int(m.group(1)) * float(m.group(2)) / 453.592),\n",
    "        (r'(\\d+)X(\\d*\\.?\\d+)\\s*L(?:T|TR?)?\\b', lambda m: int(m.group(1)) * float(m.group(2)) * get_liquid_density_per_liter(product_text)),\n",
    "        (r'\\b(\\d+(?:\\.\\d+)?)\\s*L(?:T|TR?)?\\b', lambda m: float(m.group(1)) * get_liquid_density_per_liter(product_text)),\n",
    "        (r'(\\d+)X(\\d*\\.?\\d+)\\s*ML\\b', lambda m: int(m.group(1)) * float(m.group(2)) / 1000 * get_liquid_density_per_liter(product_text)),\n",
    "        (r'\\b(\\d+(?:\\.\\d+)?)\\s*ML\\b', lambda m: float(m.group(1)) / 1000 * get_liquid_density_per_liter(product_text)),\n",
    "        (r'(\\d+)X(\\d*\\.?\\d+)\\s*PT\\b', lambda m: int(m.group(1)) * float(m.group(2)) * 1.0),\n",
    "        (r'\\b(\\d+(?:\\.\\d+)?)\\s*PT\\b', lambda m: float(m.group(1)) * 1.0),\n",
    "        (r'\\b(\\d+(?:\\.\\d+)?)\\s*OZ\\s*\\((\\d+)\\)', lambda m: float(m.group(1)) * int(m.group(2)) / 16),\n",
    "        (r'\\b(\\d+(?:\\.\\d+)?)\\s*QT\\s*\\((\\d+)\\)', lambda m: float(m.group(1)) * int(m.group(2)) * get_liquid_density(product_text) * 0.25),\n",
    "        (r'\\b(\\d+(?:\\.\\d+)?)\\s*PT\\s*\\((\\d+)\\)', lambda m: float(m.group(1)) * int(m.group(2)) * get_liquid_density(product_text) * 0.125),\n",
    "        (r'(\\d+)X(?:1/2|½)\\s*GAL\\b', lambda m: int(m.group(1)) * 0.5 * get_liquid_density(product_text)),\n",
    "        (r'(\\d+)X(\\d*\\.?\\d+(?:/\\d+)?)\\s*GAL\\b', lambda m: int(m.group(1)) * _parse_fraction(m.group(2)) * get_liquid_density(product_text)),\n",
    "        (r'\\b(\\d+(?:\\.\\d+)?(?:/\\d+)?)\\s*GAL\\b', lambda m: _parse_fraction(m.group(1)) * get_liquid_density(product_text)),\n",
    "        (r'(\\d+)X(\\d*\\.?\\d+(?:/\\d+)?)\\s*QT\\b', lambda m: int(m.group(1)) * _parse_fraction(m.group(2)) * get_liquid_density(product_text) * 0.25),\n",
    "        (r'\\b(\\d+(?:\\.\\d+)?(?:/\\d+)?)\\s*QT\\b(?!\\s*\\()', lambda m: _parse_fraction(m.group(1)) * get_liquid_density(product_text) * 0.25),\n",
    "        (r'(?:^|\\s)(\\d*\\.?\\d+)\\s*LBS?\\b', lambda m: float(m.group(1))),\n",
    "        (r'\\b(\\d+)\\s*LBS?\\b(?!\\s*MC|\\s*ATL)', lambda m: float(m.group(1))),\n",
    "        (r'\\b(\\d+(?:\\.\\d+)?)\\s*KG\\b', lambda m: float(m.group(1)) / LB_TO_KG),\n",
    "        (r'\\b(\\d+(?:\\.\\d+)?)\\s*GR?\\b', lambda m: float(m.group(1)) / 453.592),\n",
    "        (r'\\b(\\d+(?:\\.\\d+)?)\\s*OZ\\b', lambda m: float(m.group(1)) / 16),\n",
    "        (r'\\b(\\d+)\\s*BUNCH(?:ES)?\\b', lambda m: int(m.group(1)) * 0.25),\n",
    "        (r'\\b(\\d+(?:\\.\\d+)?)\\s*CT\\b', lambda m: estimate_ct_weight(float(m.group(1)), product_text, row_sheet)),\n",
    "        (r'\\b(\\d+(?:\\.\\d+)?)\\s*P(?:CS?|IECES?)\\b', lambda m: estimate_piece_weight(float(m.group(1)), product_text, row_sheet)),\n",
    "    ]\n",
    "    \n",
    "    for pattern, calc_func in patterns:\n",
    "        match = re.search(pattern, product_upper)\n",
    "        if match:\n",
    "            try:\n",
    "                result = calc_func(match)\n",
    "                if result and result > 0:\n",
    "                    debug_print(f\"'{product_text}' matched → {result:.4f} lbs\", row_sheet)\n",
    "                    return round(result, 4), \"weight_parsed\"\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    return None, \"unknown\"\n",
    "\n",
    "def estimate_ct_weight(count: float, product_text: str, row_sheet: Optional[int] = None) -> float:\n",
    "    text_lower = product_text.lower()\n",
    "    for pattern, weight in ITEM_WEIGHTS.items():\n",
    "        if re.search(pattern, text_lower):\n",
    "            total_weight = count * weight\n",
    "            debug_print(f\"CT: {count} × {weight} = {total_weight} lbs\", row_sheet)\n",
    "            return total_weight\n",
    "    if re.search(r'\\bsheets?\\b|\\bpapers?\\b', text_lower):\n",
    "        return count * 0.1\n",
    "    elif re.search(r'\\bsmalls?\\b|\\bminis?\\b', text_lower):\n",
    "        return count * 0.16\n",
    "    else:\n",
    "        return count * 0.25\n",
    "\n",
    "def estimate_piece_weight(count: float, product_text: str, row_sheet: Optional[int] = None) -> float:\n",
    "    text_lower = product_text.lower()\n",
    "    if re.search(r'\\bwatermelons?\\b|\\bpumpkins?\\b', text_lower):\n",
    "        return count * 15.0\n",
    "    elif re.search(r'\\bpineapples?\\b|\\bcantaloupes?\\b|\\bhoneydewe?\\b', text_lower):\n",
    "        return count * 3.0\n",
    "    elif re.search(r'\\bturkeys?\\b|\\bducks?\\b', text_lower):\n",
    "        return count * 12.0\n",
    "    elif re.search(r'\\bchickens?\\b|\\bfish\\b', text_lower):\n",
    "        return count * 4.0\n",
    "    else:\n",
    "        return count * 2.0\n",
    "\n",
    "# ============================================================================\n",
    "# AI INTEGRATION\n",
    "# ============================================================================\n",
    "\n",
    "def require_gpt():\n",
    "    if \"_call_gpt\" not in globals() or not callable(globals()[\"_call_gpt\"]):\n",
    "        raise RuntimeError(\"[Cell 5] _call_gpt is not defined.\")\n",
    "\n",
    "def analyze_product_with_ai(product_text: str, estimated_weight=None, row_sheet: Optional[int] = None):\n",
    "    require_gpt()\n",
    "    weight_hint = f\" (estimated total weight: {estimated_weight} lbs)\" if estimated_weight else \"\"\n",
    "    prompt = f\"\"\"Analyze this food item: {product_text}{weight_hint}\n",
    "\n",
    "Rules:\n",
    "1. Single products (cheese, butter, milk) → 100% of that food\n",
    "2. Prepared foods → break into main ingredients\n",
    "3. Think like a chef, not a chemist\n",
    "\n",
    "Return JSON: {{\"items\": [{{\"ingredient\": \"name\", \"percent\": number}}]}}\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = _call_gpt(messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You identify main food ingredients in products as they appear in recipes.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ])\n",
    "        response_text = str(response).strip()\n",
    "        json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "        if json_match:\n",
    "            response_text = json_match.group(0)\n",
    "        data = json.loads(response_text)\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            return get_fallback_ingredients(product_text)\n",
    "        valid_items = []\n",
    "        total_percent = 0\n",
    "        for item in items:\n",
    "            ingredient = str(item.get(\"ingredient\", \"\")).strip().lower()\n",
    "            if not ingredient or ingredient == \"null\":\n",
    "                continue\n",
    "            percent = float(item.get(\"percent\", 0))\n",
    "            if percent > 0:\n",
    "                valid_items.append({\"ingredient\": ingredient, \"percent\": percent})\n",
    "                total_percent += percent\n",
    "        if valid_items and abs(total_percent - 100) > 0.1:\n",
    "            for item in valid_items:\n",
    "                item[\"percent\"] = (item[\"percent\"] / total_percent) * 100\n",
    "        return valid_items if valid_items else get_fallback_ingredients(product_text)\n",
    "    except Exception as e:\n",
    "        return get_fallback_ingredients(product_text)\n",
    "\n",
    "def get_fallback_ingredients(product_text: str):\n",
    "    text_lower = product_text.lower()\n",
    "    ingredient_map = {\n",
    "        r'\\b(butter)\\b': 'butter', r'\\b(cheese)\\b': 'cheese', r'\\b(milk)\\b': 'milk',\n",
    "        r'\\b(cream)\\b': 'cream', r'\\b(yogurt)\\b': 'yogurt', r'\\b(bread)\\b': 'bread',\n",
    "        r'\\b(chicken)\\b': 'chicken', r'\\b(beef)\\b': 'beef', r'\\b(pork)\\b': 'pork',\n",
    "        r'\\b(fish)\\b': 'fish', r'\\b(oil)\\b': 'oil', r'\\b(flour)\\b': 'flour',\n",
    "    }\n",
    "    for pattern, ingredient in ingredient_map.items():\n",
    "        if re.search(pattern, text_lower):\n",
    "            return [{\"ingredient\": ingredient, \"percent\": 100.0}]\n",
    "    words = re.findall(r'\\b[a-z]{3,}\\b', text_lower)\n",
    "    ingredient = words[0] if words else \"food product\"\n",
    "    return [{\"ingredient\": ingredient, \"percent\": 100.0}]\n",
    "\n",
    "def estimate_weight_with_ai(product_text: str, qty: float, row_sheet: Optional[int] = None) -> Optional[float]:\n",
    "    text_lower = product_text.lower()\n",
    "    if 'watermelon' in text_lower:\n",
    "        result = 12.0 * qty\n",
    "        debug_print(f\"'{product_text}' AI estimate (watermelon) → {result:.4f} lbs\", row_sheet)\n",
    "        return result\n",
    "    elif 'cantaloupe' in text_lower:\n",
    "        result = 3.0 * qty\n",
    "        debug_print(f\"'{product_text}' AI estimate (cantaloupe) → {result:.4f} lbs\", row_sheet)\n",
    "        return result\n",
    "    elif 'pineapple' in text_lower:\n",
    "        result = 4.0 * qty\n",
    "        debug_print(f\"'{product_text}' AI estimate (pineapple) → {result:.4f} lbs\", row_sheet)\n",
    "        return result\n",
    "    try:\n",
    "        require_gpt()\n",
    "        prompt = f\"\"\"Estimate the total weight in pounds for this produce/food item.\n",
    "\n",
    "Product: {product_text}\n",
    "Quantity: {qty}\n",
    "\n",
    "Context:\n",
    "- CS = case (full case of produce, typically 15-50 lbs depending on item)\n",
    "- 1/2 CS = half case\n",
    "- BU/BUSH = bushel (typically 30-60 lbs depending on item)\n",
    "- Berries in cases typically weigh 10-15 lbs per case\n",
    "- Leafy greens in cases typically weigh 20-30 lbs per case\n",
    "- Root vegetables in cases typically weigh 40-60 lbs per case\n",
    "- Dense vegetables (peppers, tomatoes) in cases typically weigh 20-30 lbs per case\n",
    "\n",
    "Think step by step:\n",
    "1. Identify the product type\n",
    "2. Identify the packaging (case, half-case, bushel, count, weight unit)\n",
    "3. Estimate reasonable weight per unit\n",
    "4. Calculate total weight\n",
    "\n",
    "Provide only the final numeric answer in pounds (e.g., \"24.5\")\"\"\"\n",
    "        response = _call_gpt(messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in produce wholesale and food service packaging. You understand industry abbreviations like CS (case), BU (bushel), and typical weights for produce items in commercial foodservice.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ])\n",
    "\n",
    "        weight_match = re.search(r'(\\d+(?:\\.\\d+)?)', str(response))\n",
    "        if weight_match:\n",
    "            weight = float(weight_match.group(1))\n",
    "            if 0.1 <= weight <= 10000:\n",
    "                result = round(weight, 4)\n",
    "                debug_print(f\"'{product_text}' AI estimate (GPT) → {result:.4f} lbs\", row_sheet)\n",
    "                return result\n",
    "    except:\n",
    "        pass\n",
    "    result = 3.0 * qty\n",
    "    debug_print(f\"'{product_text}' AI estimate (fallback default) → {result:.4f} lbs\", row_sheet)\n",
    "    return result\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN PROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def process_single_product(index: int, product_text: str, df_raw: pd.DataFrame, qty_col: Optional[str]):\n",
    "    row_offset = int(os.getenv(\"FI_ROW_OFFSET\", \"2\"))\n",
    "    row_sheet = index + row_offset\n",
    "    \n",
    "    try:\n",
    "        if qty_col and qty_col in df_raw.columns:\n",
    "            qty_value = df_raw.loc[index, qty_col]\n",
    "            if pd.notna(qty_value) and qty_value != '':\n",
    "                qty_final = max(float(qty_value), 1.0)\n",
    "            else:\n",
    "                qty_final = 1.0\n",
    "        else:\n",
    "            qty_final = 1.0\n",
    "    except:\n",
    "        qty_final = 1.0\n",
    "    \n",
    "    if not product_text or not isinstance(product_text, str):\n",
    "        return None\n",
    "    product_text_clean = str(product_text).strip()\n",
    "    if len(product_text_clean) < 3 or product_text_clean.lower() in ['type01', 'type02', 'type03', 'nan', 'null']:\n",
    "        return None\n",
    "\n",
    "    per_unit_weight, source_type = extract_weight_from_text(product_text_clean, row_sheet)\n",
    "    if per_unit_weight:\n",
    "        total_weight = round(per_unit_weight * qty_final, 4)\n",
    "    else:\n",
    "        debug_print(f\"No weight pattern matched, estimating via AI\", row_sheet)\n",
    "        total_weight = estimate_weight_with_ai(product_text_clean, qty_final, row_sheet)\n",
    "        source_type = \"ai_estimated\"\n",
    "   \n",
    "    items = analyze_product_with_ai(product_text_clean, total_weight, row_sheet)\n",
    "    \n",
    "    return {\n",
    "        \"index\": index, \"product\": product_text_clean, \"qty\": qty_final,\n",
    "        \"total_weight\": total_weight, \"items\": items, \"source_type\": source_type\n",
    "    }\n",
    "\n",
    "def run_stage_1_type03_gpt_base(input_filename: str, base_name: str) -> pd.DataFrame:\n",
    "    with log_time(\"Loading input data\"):\n",
    "        df_raw = pd.read_csv(input_filename)\n",
    "        unwanted = {\"type01\", \"type02\", \"type03\"}\n",
    "        if len(df_raw.columns) > 0:\n",
    "            mask1 = ~(df_raw[df_raw.columns[0]].notna() & df_raw[df_raw.columns[0]].astype(str).str.strip().str.lower().isin(unwanted))\n",
    "        else:\n",
    "            mask1 = pd.Series([True] * len(df_raw))\n",
    "        if len(df_raw.columns) > 1:\n",
    "            mask2 = ~(df_raw[df_raw.columns[1]].notna() & df_raw[df_raw.columns[1]].astype(str).str.strip().str.lower().isin(unwanted))\n",
    "        else:\n",
    "            mask2 = pd.Series([True] * len(df_raw))\n",
    "        df_raw = df_raw[mask1 & mask2].reset_index(drop=True)\n",
    "\n",
    "    columns = {c.lower().strip(): c for c in df_raw.columns}\n",
    "    product_col = columns.get(\"product\") or columns.get(\"description\") or df_raw.columns[0]\n",
    "    qty_col = columns.get(\"qty\")\n",
    "    \n",
    "    product_series = df_raw[product_col].astype(str)\n",
    "    max_workers = int(os.getenv(\"FI_MAX_WORKERS\", \"4\"))\n",
    "    row_offset = int(os.getenv(\"FI_ROW_OFFSET\", \"2\"))\n",
    "    \n",
    "    results = []\n",
    "    with log_time(f\"Processing {len(product_series)} products\"):\n",
    "        with cf.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = {executor.submit(process_single_product, i, text, df_raw, qty_col): (i, text)\n",
    "                      for i, text in product_series.items() if pd.notna(text) and str(text).strip()}\n",
    "            for future in cf.as_completed(futures):\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    results.append(result)\n",
    "    \n",
    "    output_rows = []\n",
    "    for result in results:\n",
    "        for item in result[\"items\"]:\n",
    "            ingredient = item.get(\"ingredient\", \"\").strip() or \"food product\"\n",
    "            percent = item.get(\"percent\", 100.0)\n",
    "            ingredient_weight = round((percent * result[\"total_weight\"] / 100), 4) if result[\"total_weight\"] and percent else None\n",
    "            output_rows.append({\n",
    "                \"row_sheet\": result[\"index\"] + row_offset,\n",
    "                \"product\": result[\"product\"],\n",
    "                \"qty\": result[\"qty\"],\n",
    "                \"product_weight_lbs\": result[\"total_weight\"],\n",
    "                \"ingredient\": ingredient,\n",
    "                \"percent\": round(percent, 2) if percent else None,\n",
    "                \"ingredient_weight_lbs\": ingredient_weight,\n",
    "                \"extract_origin\": \"ai_analysis\",\n",
    "                \"source_uom_type\": result[\"source_type\"]\n",
    "            })\n",
    "\n",
    "    with log_time(\"Creating output file\"):\n",
    "        df_output = pd.DataFrame(output_rows)\n",
    "        if not df_output.empty:\n",
    "            df_output = df_output.sort_values(\"row_sheet\")\n",
    "            for col in [\"product_weight_lbs\", \"percent\", \"ingredient_weight_lbs\"]:\n",
    "                if col in df_output.columns:\n",
    "                    df_output[col] = df_output[col].round(2)\n",
    "        output_path = output_dir / Path(f\"ingredients_{base_name}.csv\")\n",
    "        df_output.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "        print(f\"[Stage 1] Wrote {len(df_output)} rows to {output_path}\")\n",
    "    return df_output\n",
    "\n",
    "try:\n",
    "    _SESSION_START_TIME\n",
    "except NameError:\n",
    "    import time\n",
    "    _SESSION_START_TIME = time.time()\n",
    "\n",
    "def run_stage_1_type03_gpt(*args, **kwargs):\n",
    "    base_name = kwargs.get(\"base\") or (args[1] if len(args) >= 2 else None)\n",
    "    output_file = Path(f\"ingredients_{base_name}.csv\") if base_name else None\n",
    "    if output_file and output_file.exists():\n",
    "        try:\n",
    "            if output_file.stat().st_mtime >= _SESSION_START_TIME:\n",
    "                return pd.read_csv(output_file)\n",
    "        except:\n",
    "            pass\n",
    "    return run_stage_1_type03_gpt_base(*args, **kwargs)\n",
    "\n",
    "print(\"=== CELL 5 v110b COMPLETE ===\")\n",
    "print(\"✓ All fixes applied\")\n",
    "print(\"✓ Ready to process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b970b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 6 — Stage 1 (General) Runner ===\n",
    "# Purpose: Execute AI ingredient extraction for type01 and type02 vendor formats.\n",
    "# Reads the procurement table, extracts MAJOR ingredients (+ percents), writes ingredients_{BASE}.csv\n",
    "# Provides log_time timings.\n",
    "# Inputs:\n",
    "#   - standardized DataFrame from Cell 3\n",
    "#   - ai_extract_ingredients() from Cell 4\n",
    "# Outputs:\n",
    "#   - ingredients_{BASE}.csv\n",
    "# Behavior:\n",
    "#   - Loops through all products, calling GPT for ingredient prediction.\n",
    "#   - Applies vegan/meatless safeguards and normalization.\n",
    "#   - Writes results to disk.\n",
    "# Notes:\n",
    "#   - This cell mirrors Cell 5 but omits item_weights.json dependency.\n",
    "#   - Overwrites existing ingredients file if START_MODE='full'.\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "import concurrent.futures as cf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json, re, os\n",
    "\n",
    "\n",
    "if file_type == 'type03':\n",
    "    print(\"Skipping this cell - type03 uses different processing\")\n",
    "    # You could also just use a pass statement or return if it's in a function\n",
    "else:\n",
    "\n",
    "    # ----------------------------\n",
    "    # Timing helper (no-op fallback)\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        log_time\n",
    "    except NameError:\n",
    "        from contextlib import contextmanager\n",
    "        @contextmanager\n",
    "        def log_time(_label: str):\n",
    "            yield\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Constants / configuration\n",
    "    # ----------------------------\n",
    "    VEGAN_TERMS = (\n",
    "        \"vegan\",\"plant-based\",\"plant based\",\"meatless\",\"vegetarian\",\n",
    "        \"dairy-free\",\"dairy free\",\"non-dairy\",\"nondairy\"\n",
    "    )\n",
    "\n",
    "    VEGAN_INGREDIENT_REPLACEMENTS = {\n",
    "    # Dairy products\n",
    "    \"yogurt\": \"non-dairy yogurt\",\n",
    "    \"milk\": \"plant milk\",\n",
    "    \"cheese\": \"vegan cheese\",\n",
    "    \"cream\": \"plant-based cream\",\n",
    "    \"butter\": \"vegan butter\",\n",
    "    \"sour cream\": \"vegan sour cream\",\n",
    "    \"cream cheese\": \"vegan cream cheese\",\n",
    "    \"ice cream\": \"non-dairy ice cream\",\n",
    "    \n",
    "    # Meat products (for vegan meat substitutes)\n",
    "    \"chicken\": \"plant-based chicken\",\n",
    "    \"beef\": \"plant-based beef\", \n",
    "    \"pork\": \"plant-based pork\",\n",
    "    \"turkey\": \"plant-based turkey\",\n",
    "    \"sausage\": \"plant-based sausage\",\n",
    "    \"bacon\": \"plant-based bacon\",\n",
    "    \n",
    "    # Other animal products\n",
    "    \"egg\": \"egg substitute\",\n",
    "    \"gelatin\": \"plant-based gelling agent\",\n",
    "    \"honey\": \"plant-based sweetener\",\n",
    "}\n",
    "    \n",
    "    ANIMAL_INGREDIENT_TOKENS = {\n",
    "        # dairy\n",
    "        \"milk\",\"dairy milk\",\"cow milk\",\"goat milk\",\"sheep milk\",\"cheese\",\"whey\",\"casein\",\"lactose\",\n",
    "        \"butter\",\"ghee\",\"cream\",\"yogurt\",\"buttermilk\",\"skim milk\",\"whole milk\",\"milk powder\",\"whey powder\",\"sour cream\",\n",
    "        # eggs & honey & gelatin\n",
    "        \"egg\",\"eggs\",\"egg white\",\"egg whites\",\"egg yolk\",\"honey\",\"gelatin\",\n",
    "        # terrestrial meats\n",
    "        \"beef\",\"pork\",\"chicken\",\"turkey\",\"lamb\",\"mutton\",\"bacon\",\"sausage\",\"venison\",\"veal\",\"ham\",\n",
    "        # fish & seafood\n",
    "        \"fish\",\"salmon\",\"tuna\",\"cod\",\"haddock\",\"pollock\",\"anchovy\",\"sardine\",\"halibut\",\"mahi\",\n",
    "        \"catfish\",\"tilapia\",\"trout\",\"bass\",\"redfish\",\"snapper\",\"shrimp\",\"prawn\",\"crab\",\"lobster\",\"oyster\",\"clam\",\"mussel\",\n",
    "    }\n",
    "    \n",
    "    # Fish species tokens for annotation\n",
    "    FISH_SPECIES = {\n",
    "        \"fish\",\"salmon\",\"tuna\",\"cod\",\"haddock\",\"pollock\",\"anchovy\",\"sardine\",\"halibut\",\"mahi\",\n",
    "        \"catfish\",\"tilapia\",\"trout\",\"bass\",\"redfish\",\"snapper\",\"hake\",\"sole\",\"flounder\",\"mackerel\",\"herring\",\"anchovies\",\"sardines\"\n",
    "    }\n",
    "    \n",
    "    # “Do not decompose” labels (top-level categories kept intact if name matches exactly)\n",
    "    DO_NOT_DECOMPOSE = set(globals().get(\"ALLOWED_CATEGORIES\", []))\n",
    "    \n",
    "    # Protected multi-word items that shouldn’t be split into subcomponents\n",
    "    PROTECTED_INGREDIENTS = {\n",
    "    \"almond milk\",\"soy milk\",\"oat milk\",\"rice milk\",\"pea milk\",\"coconut milk\",\"cashew milk\",\n",
    "    \"peanut butter\",\"ice cream\",\"tofu\",\"tempeh\",\"seitan\",\"vegan cheese alternative\",\n",
    "    \"vegan mozzarella cheese alternative\",\"non-dairy yogurt\",\"coconut yogurt\",\n",
    "    \"almond yogurt\",\"soy yogurt\",\"cashew yogurt\",\"plant-based cream\",\"vegan butter\",\n",
    "    \"plant-based chicken\",\"plant-based beef\",\"plant-based pork\",\"vegan sausage\",\n",
    "}\n",
    "    \n",
    "    # Single-item detectors (collapse obvious single-ingredient staples)\n",
    "    SINGLE_ITEM_DETECTORS: List[Tuple[re.Pattern,str]] = [\n",
    "        (re.compile(r\"\\bwater\\b\", re.I), \"water\"),\n",
    "        (re.compile(r\"\\bsugar\\b\", re.I), \"sugar\"),\n",
    "        (re.compile(r\"\\bsalt\\b\", re.I), \"salt\"),\n",
    "        (re.compile(r\"\\bflour\\b\", re.I), \"flour\"),\n",
    "        (re.compile(r\"\\bbutter\\b\", re.I), \"butter\"),\n",
    "        (re.compile(r\"\\bcream\\b\", re.I), \"cream\"),\n",
    "        (re.compile(r\"\\bmilk\\b\", re.I), \"milk\"),\n",
    "        (re.compile(r\"\\byogurt\\b\", re.I), \"yogurt\"),\n",
    "        (re.compile(r\"\\b(oil|olive oil|canola oil|vegetable oil)\\b\", re.I), \"oil\"),\n",
    "        (re.compile(r\"\\brice\\b\", re.I), \"rice\"),\n",
    "        (re.compile(r\"\\boats?\\b\", re.I), \"oats\"),\n",
    "        (re.compile(r\"\\bpasta\\b|\\bspaghetti\\b|\\bpenne\\b\", re.I), \"pasta\"),\n",
    "    ]\n",
    "    \n",
    "    # Packaging-water context (avoid collapsing to “water” just because “packed in water”)\n",
    "    PACKAGING_WATER_CONTEXT_RE = re.compile(r\"\\b(in|with)\\s+water\\b|\\bwater\\s*pack(ed)?\\b\", re.I)\n",
    "    \n",
    "    # Half & Half canonical rule\n",
    "    HALF_AND_HALF_RE = re.compile(r\"\\bhalf\\s*&\\s*half\\b|\\bhalf\\s+and\\s+half\\b\", re.I)\n",
    "    \n",
    "    # Ratios like 75/25\n",
    "    RATIO_RE = re.compile(r'(\\d{1,3})\\s*/\\s*(\\d{1,3})(?:\\s*/\\s*(\\d{1,3}))?')\n",
    "    \n",
    "    # Threshold to drop micro-ingredients post-normalization\n",
    "#    MIN_KEEP_PCT = float(os.getenv(\"FI_MIN_KEEP_PCT\", \"5\")) old version deleted\n",
    "    MIN_KEEP_PCT = CATEGORY_PERCENT_MIN\n",
    "    \n",
    "    # Vegan meat default mix (when GPT returns weak/unknown for vegan meat substitutes)\n",
    "    VEGAN_MEAT_DEFAULT = [\n",
    "        {\"ingredient\": \"legumes\",          \"percent\": 30.0},\n",
    "        {\"ingredient\": \"wheat\",            \"percent\": 30.0},\n",
    "        {\"ingredient\": \"other vegetables\", \"percent\": 30.0},\n",
    "        {\"ingredient\": \"vegetable oil\",    \"percent\": 10.0},\n",
    "    ]\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Helpers\n",
    "    # ----------------------------\n",
    "    def _norm(s: str) -> str:\n",
    "        try:\n",
    "            return norm_text(s)  # use your notebook's normalizer if present\n",
    "        except Exception:\n",
    "            t = str(s).strip().lower()\n",
    "            return re.sub(r\"\\s+\", \" \", t)\n",
    "\n",
    "    def _is_blank_product(s: Any) -> bool:\n",
    "        if s is None:\n",
    "            return True\n",
    "        t = str(s).strip()\n",
    "        if t == \"\":\n",
    "            return True\n",
    "        tl = t.lower()\n",
    "        return tl in (\"nan\", \"none\")\n",
    "    \n",
    "    def is_vegan_labeled(name: str) -> bool:\n",
    "        n = _norm(name)\n",
    "        return any(t in n for t in VEGAN_TERMS)\n",
    "    \n",
    "    def contains_animal_token(name_lc: str) -> bool:\n",
    "        return any(tok in name_lc for tok in ANIMAL_INGREDIENT_TOKENS)\n",
    "    \n",
    "    def is_allowed_plant_variant(name_lc: str) -> bool:\n",
    "        # allow explicit plant milks etc.\n",
    "        for prot in PROTECTED_INGREDIENTS:\n",
    "            if prot in name_lc:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def sanitize_vegan_dairy_naming(s: str) -> str:\n",
    "        # avoid returning bare “mozzarella” for vegan cheese; keep it generic\n",
    "        if \"mozzarella\" in s.lower() and any(k in s.lower() for k in (\"vegan\",\"plant\")):\n",
    "            return \"vegan mozzarella cheese alternative\"\n",
    "        return s\n",
    "    \n",
    "    def is_protected(ing: str) -> bool:\n",
    "        ing_lc = ing.lower()\n",
    "        return any(p in ing_lc for p in PROTECTED_INGREDIENTS)\n",
    "    \n",
    "    def detect_bone_in(text: str) -> bool:\n",
    "        t = _norm(text)\n",
    "        return bool(re.search(r\"\\bbone[-\\s]?in\\b|\\bwith bones?\\b|\\bbone[-\\s]?in\\s+skin[-\\s]?on\\b\", t))\n",
    "    \n",
    "    def detect_fish_source(text: str) -> Optional[str]:\n",
    "        t = _norm(text)\n",
    "        if re.search(r\"\\bwild\\s*[- ]?\\s*caught\\b|\\bwild\\b\", t):\n",
    "            return \"wild-caught\"\n",
    "        if re.search(r\"\\bfarm(ed|[- ]raised)?\\b|\\baquaculture\\b|\\braised\\b\", t):\n",
    "            return \"farmed\"\n",
    "        return None\n",
    "    \n",
    "    def annotate_fish_items(items: List[Dict[str,Any]], product_text: str) -> List[Dict[str,Any]]:\n",
    "        src = detect_fish_source(product_text)\n",
    "        if not src:\n",
    "            return items\n",
    "        out = []\n",
    "        for it in items:\n",
    "            name = str(it.get(\"ingredient\",\"\"))\n",
    "            nlc = name.lower()\n",
    "            # Skip sauces & non-fish\n",
    "            if \"sauce\" in nlc:\n",
    "                out.append(it); continue\n",
    "            # If already annotated, keep\n",
    "            if \"(farmed\" in nlc or \"(wild\" in nlc:\n",
    "                out.append(it); continue\n",
    "            # Annotate fish/seafood tokens\n",
    "            token = None\n",
    "            for sp in FISH_SPECIES:\n",
    "                if re.search(rf\"\\b{re.escape(sp)}\\b\", nlc):\n",
    "                    token = sp; break\n",
    "            if token:\n",
    "                it = dict(it)\n",
    "                it[\"ingredient\"] = f\"{name} ({src})\"\n",
    "            out.append(it)\n",
    "        return out\n",
    "    \n",
    "    def canonical_single_ingredient(title: str) -> Optional[str]:\n",
    "        t = title or \"\"\n",
    "        if is_vegan_labeled(t):\n",
    "            return None\n",
    "        # Respect do-not-decompose if name exactly equals an allowed category\n",
    "        n = _norm(t)\n",
    "        if n in {_norm(x) for x in DO_NOT_DECOMPOSE}:\n",
    "            return t.strip()\n",
    "        # Otherwise single-ingredient staples\n",
    "        for rx, name in SINGLE_ITEM_DETECTORS:\n",
    "            if rx.search(t):\n",
    "                # Never collapse to water if it's clearly just packaging context\n",
    "                if name == 'water' and PACKAGING_WATER_CONTEXT_RE.search(t):\n",
    "                    continue\n",
    "                return name\n",
    "        return None\n",
    "    \n",
    "    def normalize_percents(items: List[Dict[str,Any]], product_text: str) -> List[Dict[str,Any]]:\n",
    "        # If explicit ratios are in the name (e.g., 75/25), honor them if two items\n",
    "        m = RATIO_RE.search(product_text or \"\")\n",
    "        if m and len(items) == 2:\n",
    "            nums = [int(x) for x in m.groups() if x]\n",
    "            if len(nums) >= 2:\n",
    "                total = sum(nums[:2])\n",
    "                if total > 0:\n",
    "                    return [\n",
    "                        {\"ingredient\": items[0][\"ingredient\"], \"percent\": 100.0 * nums[0] / total},\n",
    "                        {\"ingredient\": items[1][\"ingredient\"], \"percent\": 100.0 * nums[1] / total},\n",
    "                    ]\n",
    "    \n",
    "        # Otherwise, if percents missing or don’t sum ~100, scale majors sensibly\n",
    "        vals = [it.get(\"percent\") for it in items if it.get(\"percent\") is not None]\n",
    "        if not vals:\n",
    "            return items\n",
    "        total = sum(p for p in vals if p is not None)\n",
    "        if total <= 0:\n",
    "            return items\n",
    "        scale = 100.0 / total\n",
    "        out = []\n",
    "        for it in items:\n",
    "            pct = it.get(\"percent\")\n",
    "            if pct is None:\n",
    "                out.append({\"ingredient\": it[\"ingredient\"], \"percent\": None})\n",
    "            else:\n",
    "                out.append({\"ingredient\": it[\"ingredient\"], \"percent\": pct * scale})\n",
    "        return out\n",
    "    \n",
    "    def drop_minors(items: List[Dict[str,Any]]) -> List[Dict[str,Any]]:\n",
    "        majors = []\n",
    "        for it in items:\n",
    "            pct = it.get(\"percent\")\n",
    "            if pct is None:\n",
    "                majors.append(it)  # keep if unsure\n",
    "            elif pct >= MIN_KEEP_PCT:\n",
    "                majors.append(it)\n",
    "        return majors or items  # never drop everything\n",
    "    \n",
    "    def looks_like_vegan_meat(text: str) -> bool:\n",
    "        if not is_vegan_labeled(text):\n",
    "            return False\n",
    "        t = _norm(text)\n",
    "        return bool(re.search(\n",
    "            r\"\\b(chicken|beef|pork|turkey|sausage|nugget|tender|patty|burger|meatball|ground|crumbles|hot\\s*dog|rib|steak|cutlet)\\b\",\n",
    "            t))\n",
    "    \n",
    "    def vegan_meat_fallback_needed(items: List[Dict[str,Any]]) -> bool:\n",
    "        if not items:\n",
    "            return True\n",
    "        # Too generic or single generic item like \"patty\"\n",
    "        generic = {\"patty\",\"burger\",\"protein\",\"meat substitute\",\"meatless patty\",\"tender\",\"nugget\"}\n",
    "        if len(items) == 1 and _norm(items[0][\"ingredient\"]) in generic:\n",
    "            return True\n",
    "        # All unknown?\n",
    "        if all(_norm(it[\"ingredient\"]) == \"unknown\" for it in items):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "# --- Vegan safeguard patch (Option A) ---\n",
    "# Provides a minimal, safe definition of enhanced_vegan_filter_and_convert()\n",
    "# and guards its invocation to prevent \"error\"/\"unknown\" failures for vegan items.\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "    def enhanced_vegan_filter_and_convert(items, product_text):\n",
    "        \"\"\"\n",
    "        Simple vegan/meatless filter and converter.\n",
    "    \n",
    "        - Removes animal-derived ingredients (meat, fish, eggs, dairy, honey, gelatin).\n",
    "        - Keeps plant-based or neutral ingredients.\n",
    "        - Returns the cleaned list; if everything is removed, returns an empty list.\n",
    "        \"\"\"\n",
    "        if not items:\n",
    "            return []\n",
    "    \n",
    "        animal_terms = (\n",
    "            \"beef\",\"pork\",\"chicken\",\"turkey\",\"lamb\",\"fish\",\"salmon\",\"tuna\",\"shrimp\",\n",
    "            \"egg\",\"cheese\",\"milk\",\"butter\",\"cream\",\"yogurt\",\"honey\",\"gelatin\"\n",
    "        )\n",
    "    \n",
    "        safe_items = []\n",
    "        for it in items:\n",
    "            ing = str(it.get(\"ingredient\",\"\")).lower()\n",
    "            # skip clear animal items\n",
    "            if any(tok in ing for tok in animal_terms):\n",
    "                continue\n",
    "            safe_items.append(it)\n",
    "    \n",
    "        return safe_items\n",
    "    \n",
    "    \n",
    "    # --- Wrap vegan-filter invocation safely ---\n",
    "    _old_ai_extract_ingredients = None\n",
    "    if 'ai_extract_ingredients' in globals():\n",
    "        _old_ai_extract_ingredients = ai_extract_ingredients  # preserve old symbol if rerun\n",
    "\n",
    "    #----------------------------\n",
    "    # ----------------------------\n",
    "    # ai_extract_ingredients (returns items, origin)\n",
    "    # ----------------------------\n",
    "    def ai_extract_ingredients(product_text: str, max_ings: int = 8) -> Tuple[List[Dict[str, Any]], str]:\n",
    "        \"\"\"\n",
    "        Return (items, origin) where items = [{\"ingredient\": str, \"percent\": float|None}, ...]\n",
    "        origin in {\"rule_halfhalf\",\"rule_single\",\"rule_fallback_vegan_meat\",\"gpt1\",\"error\",\"skip\"}\n",
    "        \"\"\"\n",
    "        if _is_blank_product(product_text):\n",
    "            return [], \"skip\"\n",
    "    \n",
    "        product_text = (product_text or \"\").strip()\n",
    "        vegan_flag = is_vegan_labeled(product_text)\n",
    "    \n",
    "        # EXACT RULE: Half & Half is 50/50 milk/cream (ignore butterfat % range)\n",
    "        if not vegan_flag and HALF_AND_HALF_RE.search(product_text):\n",
    "            return [{\"ingredient\": \"milk\", \"percent\": 50.0},\n",
    "                    {\"ingredient\": \"cream\", \"percent\": 50.0}], \"rule_halfhalf\"\n",
    "    \n",
    "        # Do-not-decompose (exact category match) OR single staples\n",
    "        single = canonical_single_ingredient(product_text)\n",
    "        if single:\n",
    "            return [{\"ingredient\": single, \"percent\": 100.0}], \"rule_single\"\n",
    "    \n",
    "        # --- Build the LLM prompt (your established guidance) ---\n",
    "        base_rules = [\n",
    "            \"Return ingredients for the product below as a JSON object with key `items`.\",\n",
    "            \"Each item must be {\\\"ingredient\\\": <string>, \\\"percent\\\": <number or null>}.\",\n",
    "            f\"Limit to at most {max_ings} items.\",\n",
    "            \"Report ONLY major ingredients by mass. OMIT micro-ingredients likely under \"\n",
    "            f\"{MIN_KEEP_PCT}% (e.g., salt, acids, leaveners, stabilizers, spices, flavors), unless the product has only 1–2 ingredients.\",\n",
    "            \"If the product name specifies blend ratios (e.g., 75/25), reflect those percents exactly.\",\n",
    "            \"If unsure of exact amounts, provide realistic approximate percents for MAJOR ingredients; do NOT evenly split many items.\",\n",
    "            \"Do NOT decompose protected items; keep them intact (e.g., 'almond milk', 'ice cream', 'peanut butter', 'tofu', 'tempeh', 'seitan').\",\n",
    "            \"IF the product name denotes a single-ingredient staple (milk, cream, butter, yogurt, a single specific cheese, plain oil, sugar, flour, salt, rice, oats, pasta, water, vinegar), output EXACTLY one item at 100 and do NOT list additives.\",\n",
    "            \"IF the product name includes 'half & half' or 'half and half', output exactly two items: 'milk' 50% and 'cream' 50%.\",\n",
    "            # === added to reduce unknowns ===\n",
    "            \"If the product clearly is a non-food item (e.g. napkins, paper towels, cups, containers, foil, plastic wrap), return one item {\\\"ingredient\\\": \\\"unknown\\\", \\\"percent\\\": 100}.  Otherwise, never return 'unknown'—instead infer the most typical major ingredients for that food product.\",\n",
    "        ]\n",
    "        vegan_rules = [\n",
    "            \"IMPORTANT: The product is labeled vegan/plant-based/vegetarian/non-dairy.\",\n",
    "            \"Do NOT include any animal-derived ingredients (meat, fish/seafood, eggs, dairy, gelatin, honey).\",\n",
    "            \"If a 'milk' is implied, interpret as a plant milk and name explicitly (e.g., 'almond milk').\",\n",
    "            \"Avoid naming dairy cheeses directly; if needed, use 'vegan cheese alternative' terminology.\",\n",
    "            \"IF the product name indicates vegan or imitation cheese and GPT would otherwise return 'unknown', \"\n",
    "            \"infer a plausible base composition such as water (~45%), vegetable oil (~23%), potato starch (~22%), \"\n",
    "            \"and legumes (~10%), or similar plant-based ingredients that form vegan cheese.\",\n",
    "        ] if vegan_flag else []\n",
    "    \n",
    "        protected_note = \"Protected examples:\\n\" + \", \".join(sorted(PROTECTED_INGREDIENTS))\n",
    "        system_msg = (\n",
    "            \"You extract food ingredients as structured JSON for downstream life-cycle analysis. \"\n",
    "            \"Prioritize MAJOR ingredients and realistic percents; omit micro-ingredients under the threshold.\"\n",
    "        )\n",
    "        user_msg = \"\\n\".join(base_rules + vegan_rules + [\n",
    "            \"\", protected_note, \"\", \"Product text:\", product_text, \"\",\n",
    "            \"Respond ONLY with JSON like: {\\\"items\\\":[{\\\"ingredient\\\":\\\"...\\\",\\\"percent\\\":12.3}, ...]}\",\n",
    "        ])\n",
    "    \n",
    "        gpt_called = True\n",
    "        try:\n",
    "            # NOTE: uses the notebook's existing _call_gpt defined earlier\n",
    "            raw = _call_gpt(\n",
    "                messages=[{\"role\": \"system\", \"content\": system_msg},\n",
    "                          {\"role\": \"user\", \"content\": user_msg}]\n",
    "            )\n",
    "        except Exception:\n",
    "            raw = \"\"\n",
    "            gpt_called = False\n",
    "    \n",
    "        # Parse JSON\n",
    "        items: List[Dict[str, Any]] = []\n",
    "        if raw:\n",
    "            try:\n",
    "                obj = json.loads(raw)\n",
    "                cand = obj.get(\"items\", [])\n",
    "                if isinstance(cand, list):\n",
    "                    for it in cand:\n",
    "                        ing = str(it.get(\"ingredient\", \"\")).strip()\n",
    "                        if not ing:\n",
    "                            continue\n",
    "                        pct = it.get(\"percent\", None)\n",
    "                        try:\n",
    "                            pct = float(pct) if pct is not None else None\n",
    "                        except Exception:\n",
    "                            pct = None\n",
    "                        # Snap protected phrases\n",
    "                        if is_protected(ing):\n",
    "                            for prot in PROTECTED_INGREDIENTS:\n",
    "                                if prot in ing.lower():\n",
    "                                    ing = prot\n",
    "                                    break\n",
    "                        items.append({\"ingredient\": ing, \"percent\": pct})\n",
    "            except Exception:\n",
    "                items = []\n",
    "    \n",
    "        origin = \"gpt1\" if gpt_called else \"error\"\n",
    "\n",
    "        def _is_all_unknown(lst):\n",
    "            return (not lst) or all(_norm(it.get(\"ingredient\",\"\")) == \"unknown\" for it in lst)\n",
    "        \n",
    "        if _is_all_unknown(items):\n",
    "            try:\n",
    "                fb_user = (\n",
    "                    \"Return realistic MAJOR ingredients (avoid 'unknown'). \"\n",
    "                    f\"Limit to 5 items, omit micro-ingredients under {MIN_KEEP_PCT}%. \"\n",
    "                    \"If the product is vegan/plant-based/imitation, use typical substitutes. \"\n",
    "                    f\"Product text: {product_text}\\n\"\n",
    "                    'Respond ONLY as JSON like {\"items\":[{\"ingredient\":\"...\",\"percent\":12.3}, ...]}'\n",
    "                )\n",
    "                raw2 = _call_gpt(messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You infer typical ingredient compositions for institutional foods.\"},\n",
    "                    {\"role\": \"user\", \"content\": fb_user}\n",
    "                ])\n",
    "                try:\n",
    "                    obj2 = json.loads(raw2)\n",
    "                except Exception:\n",
    "                    m = re.search(r\"\\{[\\s\\S]*\\}\", raw2 or \"\")\n",
    "                    obj2 = json.loads(m.group(0)) if m else {}\n",
    "                cand2 = obj2.get(\"items\", [])\n",
    "                new_items = []\n",
    "                if isinstance(cand2, list):\n",
    "                    for it in cand2:\n",
    "                        ing = str(it.get(\"ingredient\",\"\")).strip()\n",
    "                        if not ing:\n",
    "                            continue\n",
    "                        pct = it.get(\"percent\", None)\n",
    "                        try:\n",
    "                            pct = float(pct) if pct is not None else None\n",
    "                        except Exception:\n",
    "                            pct = None\n",
    "                        # snap protected phrases\n",
    "                        if is_protected(ing):\n",
    "                            for prot in PROTECTED_INGREDIENTS:\n",
    "                                if prot in ing.lower():\n",
    "                                    ing = prot\n",
    "                                    break\n",
    "                        new_items.append({\"ingredient\": ing, \"percent\": pct})\n",
    "                if new_items and not _is_all_unknown(new_items):\n",
    "                    items = new_items\n",
    "                    origin = \"gpt_fallback\"\n",
    "            except Exception:\n",
    "                # keep prior items/origin if fallback fails\n",
    "                pass\n",
    "\n",
    "\n",
    "        # === added fallback to avoid \"unknown\" ===\n",
    "        def _is_all_unknown(lst):\n",
    "            return all(_norm(it[\"ingredient\"]) == \"unknown\" for it in lst)\n",
    "\n",
    "        if (not items) or _is_all_unknown(items):\n",
    "            try:\n",
    "                fallback_prompt = f\"\"\"\n",
    "You are a food product analyst estimating typical compositions for institutional foods.\n",
    "\n",
    "The following product name was too vague for prior analysis:\n",
    "\"{product_text}\"\n",
    "\n",
    "Task:\n",
    "Infer the most typical MAJOR ingredients and their approximate percentages by mass.\n",
    "Avoid \"unknown\" or \"other\". If the product name is generic (e.g., 'Chicken Patty', 'Fish Stick'),\n",
    "use realistic, generic ingredients (e.g., 'chicken', 'breading', 'oil').\n",
    "Limit to 5 ingredients.\n",
    "\n",
    "Respond ONLY as JSON:\n",
    "{{\"items\":[{{\"ingredient\":\"...\",\"percent\":...}},...]}}\n",
    "\"\"\"\n",
    "                raw2 = _call_gpt(\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You estimate typical ingredient compositions for common institutional food products.\"},\n",
    "                        {\"role\": \"user\", \"content\": fallback_prompt}\n",
    "                    ]\n",
    "                )\n",
    "                obj2 = json.loads(raw2)\n",
    "                cand2 = obj2.get(\"items\", [])\n",
    "                if isinstance(cand2, list) and cand2:\n",
    "                    items = []\n",
    "                    for it in cand2:\n",
    "                        ing = str(it.get(\"ingredient\", \"\")).strip()\n",
    "                        pct = it.get(\"percent\", None)\n",
    "                        try:\n",
    "                            pct = float(pct) if pct is not None else None\n",
    "                        except Exception:\n",
    "                            pct = None\n",
    "                        if ing:\n",
    "                            items.append({\"ingredient\": ing, \"percent\": pct})\n",
    "                    origin = \"gpt_fallback\"\n",
    "            except Exception:\n",
    "                pass\n",
    "        # === end added fallback ===\n",
    "    \n",
    "        # Vegan post-filter & naming sanitizer\n",
    "        if vegan_flag and items:\n",
    "            try:\n",
    "                items = enhanced_vegan_filter_and_convert(items, product_text)\n",
    "            except Exception as e:\n",
    "                print(f\"[warn] vegan filter failed for: {product_text[:80]} ({e})\")\n",
    "            if not items and looks_like_vegan_meat(product_text):\n",
    "                return VEGAN_MEAT_DEFAULT.copy(), \"rule_fallback_vegan_meat\"\n",
    "    \n",
    "        # Vegan meat fallback if GPT weak/unknown for vegan-meatlike item\n",
    "        if looks_like_vegan_meat(product_text) and vegan_meat_fallback_needed(items):\n",
    "            return VEGAN_MEAT_DEFAULT.copy(), \"rule_fallback_vegan_meat\"\n",
    "    \n",
    "        # Normalize, annotate fish, and drop minors\n",
    "        items = normalize_percents(items, product_text)\n",
    "        items = annotate_fish_items(items, product_text)\n",
    "        items = drop_minors(items)\n",
    "\n",
    "        if not items and vegan_flag and \"cheese\" in product_text.lower():\n",
    "            return [\n",
    "                {\"ingredient\": \"water\", \"percent\": 45.0},\n",
    "                {\"ingredient\": \"vegetable oil\", \"percent\": 23.0},\n",
    "                {\"ingredient\": \"potato starch\", \"percent\": 22.0},\n",
    "                {\"ingredient\": \"legumes\", \"percent\": 10.0},\n",
    "            ], \"rule_fallback_vegan_cheese\"\n",
    "\n",
    "        if not items:\n",
    "            return [{\"ingredient\": \"unknown\", \"percent\": 100.0}], origin\n",
    "        return items, origin\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Weight extraction helper\n",
    "    # ----------------------------\n",
    "    def _extract_row_weights(src_row: pd.Series) -> Tuple[Optional[float], Optional[float]]:\n",
    "        \"\"\"Return (product_weight_lbs, weight_kg) if present on the source row.\"\"\"\n",
    "        p_lbs = None\n",
    "        wkg = None\n",
    "        for cand in (\"ingredient_weight_lbs\",\"product_weight_lbs\",\"weight_lbs\"):\n",
    "            if cand in src_row.index and pd.notna(src_row[cand]):\n",
    "                try:\n",
    "                    p_lbs = float(src_row[cand])\n",
    "                    break\n",
    "                except Exception:\n",
    "                    pass\n",
    "        if (\"weight_kg\" in src_row.index) and pd.notna(src_row[\"weight_kg\"]):\n",
    "            try:\n",
    "                wkg = float(src_row[\"weight_kg\"])\n",
    "            except Exception:\n",
    "                pass\n",
    "        return p_lbs, wkg\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Stage 1 runner (with log_time)\n",
    "    # ----------------------------\n",
    "    def _find_product_col(df: pd.DataFrame) -> str:\n",
    "        for c in df.columns:\n",
    "            if _norm(c) in (\"product\",\"product name\",\"item\",\"description\",\"name\"):\n",
    "                return c\n",
    "        return df.columns[0]\n",
    "    \n",
    "    def run_stage_1(input_filename: str, file_type: str, base: str) -> pd.DataFrame:\n",
    "        with log_time(\"S1.0 load + standardize\"):\n",
    "            try:\n",
    "                std = load_and_standardize(input_filename, file_type)\n",
    "                df = std\n",
    "                prod_col = \"product\"\n",
    "                series = df[prod_col].astype(str)\n",
    "            except Exception:\n",
    "                df = pd.read_csv(input_filename)\n",
    "                prod_col = _find_product_col(df)\n",
    "                series = df[prod_col].astype(str)\n",
    "   \n",
    "        # Filter out unwanted type markers before processing\n",
    "        unwanted = {\"type01\", \"type02\", \"type03\"}\n",
    "        \n",
    "        # Create masks for first two columns (typically product and description)\n",
    "        if len(df.columns) > 0:\n",
    "            mask1 = ~(df[df.columns[0]].notna() & \n",
    "                     df[df.columns[0]].astype(str).str.strip().str.lower().isin(unwanted))\n",
    "        else:\n",
    "            mask1 = pd.Series([True] * len(df))\n",
    "        \n",
    "        if len(df.columns) > 1:\n",
    "            mask2 = ~(df[df.columns[1]].notna() & \n",
    "                     df[df.columns[1]].astype(str).str.strip().str.lower().isin(unwanted))\n",
    "        else:\n",
    "            mask2 = pd.Series([True] * len(df))\n",
    "        \n",
    "        # Apply both masks\n",
    "        df = df[mask1 & mask2].reset_index(drop=True)\n",
    "        series = df[prod_col].astype(str)\n",
    "\n",
    "        # === added: skip NaN/blank product rows BEFORE submitting to GPT ===\n",
    "        keep_mask = df[prod_col].notna() & df[prod_col].astype(str).str.strip().ne(\"\")\n",
    "        df = df.loc[keep_mask]\n",
    "        series = df[prod_col].astype(str)\n",
    "        # === end added ===\n",
    "        # --- Skip empty, blank, or obvious non-values before submitting to GPT ---\n",
    "        mask_valid = (\n",
    "            series.notna()\n",
    "            & series.str.strip().ne(\"\")\n",
    "            & ~series.str.lower().isin([\"nan\", \"none\"])\n",
    "        )\n",
    "        series = series[mask_valid]\n",
    "        \n",
    "        # Settings\n",
    "        max_workers = int(os.getenv(\"FI_MAX_WORKERS\", \"8\"))\n",
    "        max_ings = int(os.getenv(\"FI_MAX_INGS\", \"8\"))\n",
    "        ROW_OFFSET = int(os.getenv(\"FI_ROW_OFFSET\", \"2\"))  # header row=1, first product=2\n",
    "    \n",
    "        def _one(idx_val: int, text: str) -> Tuple[List[Dict[str, Any]], str]:\n",
    "            try:\n",
    "                return ai_extract_ingredients(text, max_ings=max_ings)\n",
    "            except Exception:\n",
    "                return [{\"ingredient\": \"unknown\", \"percent\": 100.0}], \"error\"\n",
    "    \n",
    "        rows: List[Dict[str, Any]] = []\n",
    "    \n",
    "        with log_time(f\"S1.1 submit {len(series)} items to pool (max_workers={max_workers})\"):\n",
    "            with cf.ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "                futures = {\n",
    "                    ex.submit(_one, i, t): (i, t)\n",
    "                    for i, t in series.items()\n",
    "                    if (pd.notna(t) and str(t).strip())\n",
    "                }\n",
    "    \n",
    "                with log_time(\"S1.2 collect results from pool\"):\n",
    "                    for fut in cf.as_completed(futures):\n",
    "                        i, t = futures[fut]\n",
    "                        items, origin = fut.result()\n",
    "    \n",
    "                        src_row = df.loc[i] if i in df.index else pd.Series(dtype=object)\n",
    "                        p_lbs, wkg = _extract_row_weights(src_row)\n",
    "                        bone_in = detect_bone_in(str(t))\n",
    "                        qty_val = None\n",
    "                        if \"qty\" in src_row.index and pd.notna(src_row[\"qty\"]):\n",
    "                            qty_val = src_row[\"qty\"]\n",
    "    \n",
    "                        # Prefer standardized product name if available\n",
    "                        prod_name = (str(src_row[\"product\"])\n",
    "                                     if (\"product\" in src_row.index and pd.notna(src_row[\"product\"]))\n",
    "                                     else str(t))\n",
    "    \n",
    "                        base_fields = {\n",
    "                            \"__row\": i,\n",
    "                            \"row_sheet\": int(i) + ROW_OFFSET,\n",
    "                            \"product\": prod_name,\n",
    "                            \"extract_origin\": origin,\n",
    "#                            \"bone_in\": bool(bone_in),\n",
    "                            \"bone_in\": \"True\" if bone_in else \"\",\n",
    "                        }\n",
    "                        if qty_val is not None:\n",
    "                            base_fields[\"qty\"] = qty_val\n",
    "                        if p_lbs is not None:\n",
    "                            base_fields[\"product_weight_lbs\"] = p_lbs\n",
    "                        if wkg is not None:\n",
    "                            base_fields[\"weight_kg\"] = wkg\n",
    "    \n",
    "                        for it in items:\n",
    "                            rec = dict(base_fields)\n",
    "                            rec[\"ingredient\"] = it[\"ingredient\"]\n",
    "#                            rec[\"percent\"] = it.get(\"percent\", None)\n",
    "                            val = it.get(\"percent\", None)\n",
    "                            rec[\"percent\"] = round(val, 2) if isinstance(val, (int, float)) else val\n",
    "                            rows.append(rec)\n",
    "\n",
    "\n",
    "        with log_time(\"S1.3 build DataFrame\"):\n",
    "            df_ing = pd.DataFrame(rows)\n",
    "    \n",
    "            # Ensure row_sheet exists (fallback to __row + offset)\n",
    "            if \"row_sheet\" not in df_ing.columns:\n",
    "                if \"__row\" in df_ing.columns:\n",
    "                    df_ing[\"row_sheet\"] = df_ing[\"__row\"].astype(int) + ROW_OFFSET\n",
    "                else:\n",
    "                    df_ing = df_ing.reset_index(drop=False).rename(columns={\"index\": \"__row\"})\n",
    "                    df_ing[\"row_sheet\"] = df_ing[\"__row\"].astype(int) + ROW_OFFSET\n",
    "    \n",
    "            # Calculate ingredient_weight_lbs and format decimal columns\n",
    "            if 'percent' in df_ing.columns and 'product_weight_lbs' in df_ing.columns:\n",
    "                df_ing['ingredient_weight_lbs'] = df_ing['percent'] * df_ing['product_weight_lbs'] / 100\n",
    "                \n",
    "                # Round to 2 decimal places\n",
    "                df_ing['percent'] = df_ing['percent'].round(2)\n",
    "                df_ing['ingredient_weight_lbs'] = df_ing['ingredient_weight_lbs'].round(2)\n",
    "                df_ing['product_weight_lbs'] = df_ing['product_weight_lbs'].round(2)\n",
    "    \n",
    "            # Column order (keep one 'product' column; hide __row in file) - add ingredient_weight_lbs\n",
    "# optional add qty (remove next line)           preferred = [\"row_sheet\",\"product\",\"qty\",\"product_weight_lbs\",\"ingredient\",\"percent\",\"ingredient_weight_lbs\",\"bone_in\",\"extract_origin\"]\n",
    "            preferred = [\"row_sheet\",\"product\",\"product_weight_lbs\",\"ingredient\",\"percent\",\"ingredient_weight_lbs\",\"extract_origin\",\"bone_in\"]\n",
    "            ordered = [c for c in preferred if c in df_ing.columns]\n",
    "            ordered += [c for c in df_ing.columns if c not in set(ordered + [\"__row\"])]\n",
    "            df_ing = df_ing[ordered].sort_values(\"row_sheet\", kind=\"stable\")\n",
    "    \n",
    "        with log_time(\"S1.4 write ingredients CSV\"):\n",
    "            out_path = output_dir / Path(f\"ingredients_{base}.csv\")\n",
    "            df_ing.drop(columns=[\"__row\"], errors=\"ignore\").to_csv(out_path, index=False)\n",
    "            print(f\"[Stage 1] Wrote {len(df_ing)} rows to {out_path} (submitted {len(series)} products)\")\n",
    "    \n",
    "        return df_ing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f614051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 7 — Stage 2: Categorization ===\n",
    "# Purpose: Map each ingredient to a standardized food category using a mix of deterministic and AI-assisted logic.\n",
    "# Inputs:\n",
    "#   - ingredients_{BASE}.csv (from Stage 1), factors.csv (to derive ALLOWED_CATEGORIES if not provided)\n",
    "#   - foodcategories.json (category dictionary)\n",
    "# Dependencies:\n",
    "#   - AI fallback function (optional)\n",
    "#   - normalization utilities (_norm)\n",
    "# Outputs:\n",
    "#   - categories_{BASE}.csv — categorized ingredient data\n",
    "#   - updated foodcategories.json (optional append)\n",
    "# Behavior:\n",
    "#   - Performs exact and fuzzy matching.\n",
    "#   - Calls GPT for unmatched ingredients if ALLOW_AI_FALLBACK is True.\n",
    "#   - Logs all newly learned ingredient–category pairs for review.\n",
    "# Notes:\n",
    "#   - Overwrites existing categories_{BASE}.csv if START_MODE='full' or 'after_ingredients'.\n",
    "#   - Skips if START_MODE='after_categories'.\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json, re, os\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# --- Optional timing (no-op if you didn't add it in Utilities) ---\n",
    "try:\n",
    "    log_time\n",
    "except NameError:\n",
    "    from contextlib import contextmanager\n",
    "    @contextmanager\n",
    "    def log_time(_): \n",
    "        yield\n",
    "\n",
    "# ----------------------------fing - \n",
    "# Config & helpers\n",
    "# ----------------------------\n",
    "VEGAN_TERMS = (\n",
    "    \"vegan\",\"plant-based\",\"plant based\",\"meatless\",\"vegetarian\",\n",
    "    \"dairy-free\",\"dairy free\",\"non-dairy\",\"nondairy\"\n",
    ")\n",
    "\n",
    "DAIRY_CATEGORIES = {\n",
    "    \"dairy milk\",\"skim milk\",\"whole milk\",\"concentrated milk\",\"milk powder\",\"whey powder\",\n",
    "    \"butter\",\"ghee\",\"cream\",\"sour cream\",\"ice cream\",\"cheese\",\"dairy cheese\",\n",
    "    \"yogurt\",\"low fat yogurt\",\"buttermilk\",\"lactose powder\"\n",
    "}\n",
    "\n",
    "PLANT_MILK_WORDS = {\n",
    "    \"almond\",\"soy\",\"oat\",\"rice\",\"pea\",\"coconut\",\"cashew\",\"hemp\",\"hazelnut\",\"macadamia\"\n",
    "}\n",
    "\n",
    "FUZZY_CUTOFF = 0.92  # high cutoff to avoid wrong hits\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    \"\"\"Use your norm_text if defined, else lowercase/strip/collapse spaces.\"\"\"\n",
    "    try:\n",
    "        return norm_text(s)\n",
    "    except Exception:\n",
    "        t = str(s).strip().lower()\n",
    "        return re.sub(r\"\\s+\", \" \", t)\n",
    "\n",
    "def _load_allowed_categories(factors_csv: Optional[str] = None) -> Tuple[List[str], Dict[str,str]]:\n",
    "    \"\"\"Return (allowed_list, norm->display map).\"\"\"\n",
    "    allowed = None\n",
    "    if \"ALLOWED_CATEGORIES\" in globals():\n",
    "        try:\n",
    "            allowed = list(ALLOWED_CATEGORIES)\n",
    "        except Exception:\n",
    "            allowed = None\n",
    "    if allowed is None:\n",
    "        cand = factors_csv or globals().get(\"factors_filename\") or \"factors.csv\"\n",
    "        fac = pd.read_csv(cand)\n",
    "        if \"category\" not in fac.columns:\n",
    "            fac = fac.rename(columns={fac.columns[0]: \"category\"})\n",
    "        allowed = fac[\"category\"].dropna().astype(str).tolist()\n",
    "    key_map = {_norm(c): c for c in allowed}\n",
    "    return allowed, key_map\n",
    "\n",
    "def _vegan_hint(product_key: str) -> bool:\n",
    "    return any(t in product_key for t in VEGAN_TERMS)\n",
    "\n",
    "def _is_dairy(cat_display: str) -> bool:\n",
    "    return _norm(cat_display) in {_norm(x) for x in DAIRY_CATEGORIES}\n",
    "\n",
    "def _milk_heuristic(ingredient_key: str, allowed_key2disp: Dict[str,str]) -> Optional[str]:\n",
    "    \"\"\"If ingredient looks like '<plant> milk' and plant is known, map to that milk category if allowed.\"\"\"\n",
    "    m = re.search(r\"\\b([a-z][a-z ]+)\\s+milk\\b\", ingredient_key)\n",
    "    if not m:\n",
    "        return None\n",
    "    plant = m.group(1).strip().split()[-1]\n",
    "    if plant in PLANT_MILK_WORDS:\n",
    "        key = _norm(f\"{plant} milk\")\n",
    "        return allowed_key2disp.get(key)\n",
    "    return None\n",
    "\n",
    "def _fuzzy_best(label_norm: str, allowed_norm: List[str]) -> Tuple[Optional[str], float]:\n",
    "    best = 0.0\n",
    "    hit = None\n",
    "    for cand in allowed_norm:\n",
    "        r = SequenceMatcher(None, label_norm, cand).ratio()\n",
    "        if r > best:\n",
    "            best = r; hit = cand\n",
    "    return hit, best\n",
    "\n",
    "# ---- Minimal JSON dict cache (ingredient -> category), append-only ----\n",
    "def _load_foodcats_map(path: Path) -> Dict[str, str]:\n",
    "    \"\"\"Load simple dict {ingredient -> category}. If missing/invalid, return {}. Keys normalized.\"\"\"\n",
    "    if not path.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        data = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "        if not isinstance(data, dict):\n",
    "            return {}\n",
    "        out = {}\n",
    "        for k, v in data.items():\n",
    "            nk = _norm(k)\n",
    "            if nk and str(v).strip():\n",
    "                out.setdefault(nk, str(v).strip())  # keep first; do not overwrite\n",
    "        return out\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def _append_foodcats_map(path: Path, new_rows: pd.DataFrame) -> int:\n",
    "    \"\"\"Append-only: add new {ingredient_key -> category}. Never overwrites existing keys.\n",
    "       Skips empty categories and 'unknown'. Returns the number of keys appended.\"\"\"\n",
    "    if new_rows is None or new_rows.empty:\n",
    "        return 0\n",
    "    cur = _load_foodcats_map(path)\n",
    "    before = len(cur)\n",
    "    for _, r in new_rows.iterrows():\n",
    "        ik = _norm(r.get(\"ingredient_key\", \"\"))\n",
    "        cat = str(r.get(\"category\", \"\")).strip()\n",
    "        if not ik or not cat or cat.lower() == \"unknown\":\n",
    "            continue\n",
    "        if ik not in cur:\n",
    "            cur[ik] = cat\n",
    "    added = len(cur) - before\n",
    "    if added > 0:\n",
    "        path.write_text(json.dumps(cur, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    return added\n",
    "\n",
    "\n",
    "# ---- Per-run CSV cache for (ingredient_key, product_key) ----\n",
    "def _load_run_cache(path: Path) -> pd.DataFrame:\n",
    "    if path.exists():\n",
    "        df = pd.read_csv(path)\n",
    "        need = [\"ingredient_key\",\"product_key\",\"category\",\"match_method\"]\n",
    "        for c in need:\n",
    "            if c not in df.columns:\n",
    "                df[c] = \"\"\n",
    "        return df[need].drop_duplicates()\n",
    "    return pd.DataFrame(columns=[\"ingredient_key\",\"product_key\",\"category\",\"match_method\"])\n",
    "\n",
    "def _update_run_cache(path: Path, rows: pd.DataFrame) -> None:\n",
    "    if rows is None or rows.empty:\n",
    "        return\n",
    "    cur = _load_run_cache(path)\n",
    "    merged = pd.concat([cur, rows[[\"ingredient_key\",\"product_key\",\"category\",\"match_method\"]]], ignore_index=True)\n",
    "    merged = merged.drop_duplicates(subset=[\"ingredient_key\",\"product_key\"], keep=\"last\")\n",
    "    merged.to_csv(path, index=False)\n",
    "\n",
    "# ---- Strict GPT categorizer (must return exactly one allowed label) ----\n",
    "def _call_gpt_categorize(ingredient: str, product: str, allowed: List[str], vegan_guard: bool) -> Optional[str]:\n",
    "    \"\"\"Requires `_call_gpt` (defined in Utilities/Cell 5). Accepts only an allowed label.\"\"\"\n",
    "    if \"_call_gpt\" not in globals():\n",
    "        return None\n",
    "    used = [c for c in allowed if not (_is_dairy(c) and vegan_guard)] or allowed\n",
    "    if not used:\n",
    "        return None\n",
    "    allowed_map = {c.lower(): c for c in used}\n",
    "\n",
    "    system_msg = (\n",
    "        \"You classify one ingredient into exactly one category from a provided list. \"\n",
    "        \"Reply with ONLY the chosen category string—no extra words.\"\n",
    "    )\n",
    "    guard_msg = (\n",
    "        \"Do NOT choose any dairy categories (milk, cheese, butter, cream, yogurt, whey, casein); \"\n",
    "        \"the product indicates vegan/plant-based/non-dairy.\"\n",
    "        if vegan_guard else \"\"\n",
    "    )\n",
    "    user_msg = (\n",
    "        \"Choose exactly ONE category from the list that best matches the ingredient.\\n\"\n",
    "        \"Rules:\\n\"\n",
    "        \"  • Reply with ONLY the chosen category string (character-for-character).\\n\"\n",
    "        \"  • If none is perfect, choose the closest reasonable option from the list.\\n\"\n",
    "        f\"  • {guard_msg}\\n\\n\"\n",
    "        f\"Ingredient: {ingredient}\\n\"\n",
    "        f\"Product context: {product}\\n\\n\"\n",
    "        \"Allowed categories:\\n\" + \"\\n\".join(f\"- {c}\" for c in used)\n",
    "    )\n",
    "    try:\n",
    "        out = _call_gpt(\n",
    "            messages=[{\"role\":\"system\",\"content\":system_msg},\n",
    "                      {\"role\":\"user\",\"content\":user_msg}],\n",
    "            temperature=0\n",
    "        )\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    if not out:\n",
    "        return None\n",
    "    reply = out.strip().strip('\"').strip(\"'\")\n",
    "    return allowed_map.get(reply.lower())  # None if not exact allowed label\n",
    "\n",
    "# ----------------------------\n",
    "# Main: Stage 2\n",
    "# ----------------------------\n",
    "def run_stage_2_base(base: str, factors_csv: Optional[str] = None):\n",
    "    with log_time(\"S2.0 load ingredients\"):\n",
    "        ing_path = output_dir / Path(f\"ingredients_{base}.csv\")\n",
    "        df = pd.read_csv(ing_path)\n",
    "\n",
    "        # Ensure 'product' and 'row_sheet'\n",
    "        if \"product\" not in df.columns and \"product_text\" in df.columns:\n",
    "            df = df.rename(columns={\"product_text\":\"product\"})\n",
    "        if \"product\" not in df.columns:\n",
    "            # last resort fallback to first column\n",
    "            df[\"product\"] = df.iloc[:,0].astype(str)\n",
    "        df[\"product\"] = df[\"product\"].astype(str)\n",
    "\n",
    "        if \"row_sheet\" not in df.columns:\n",
    "            df = df.reset_index(drop=False).rename(columns={\"index\":\"__row\"})\n",
    "            row_offset = int(os.getenv(\"FI_ROW_OFFSET\",\"2\"))\n",
    "            df[\"row_sheet\"] = df[\"__row\"].astype(int) + row_offset\n",
    "\n",
    "        # Normalize once\n",
    "        df[\"ingredient\"] = df[\"ingredient\"].astype(str)\n",
    "        df[\"ingredient_key\"] = df[\"ingredient\"].map(_norm)\n",
    "        df[\"product_key\"]    = df[\"product\"].map(_norm)\n",
    "\n",
    "        print(f\"[S2] rows={len(df):,}  unique (ingredient,product)={df[['ingredient_key','product_key']].drop_duplicates().shape[0]:,}\")\n",
    "\n",
    "    with log_time(\"S2.1 allowed & maps\"):\n",
    "        allowed, allowed_key2disp = _load_allowed_categories(factors_csv)\n",
    "        allowed_norm = list(allowed_key2disp.keys())\n",
    "        # Optional curated exact map\n",
    "        global_map = {}\n",
    "        if \"CATEGORY_MAP\" in globals():\n",
    "            try:\n",
    "                global_map = {_norm(k): v for k, v in dict(CATEGORY_MAP).items()}\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    with log_time(\"S2.2 load caches\"):\n",
    "        json_cache_path = Path(\"foodcategories.json\")\n",
    "        json_map = _load_foodcats_map(json_cache_path)  # {ingredient_key -> category}\n",
    "        csv_cache_path  = Path(f\"category_cache_{base}.csv\")\n",
    "        run_cache = _load_run_cache(csv_cache_path)\n",
    "        appended_json_total = 0\n",
    "\n",
    "    # Work copy\n",
    "    df_cat = df.copy()\n",
    "    df_cat[\"category\"] = pd.NA\n",
    "    df_cat[\"match_method\"] = pd.NA\n",
    "\n",
    "    # Layer 1: exact CATEGORY_MAP (curated)\n",
    "    with log_time(\"S2.3 exact CATEGORY_MAP\"):\n",
    "        if global_map:\n",
    "            hit = df_cat[\"ingredient_key\"].map(global_map)\n",
    "            m = hit.notna()\n",
    "            df_cat.loc[m, \"category\"] = hit[m]\n",
    "            df_cat.loc[m, \"match_method\"] = \"map_exact\"\n",
    "\n",
    "    # Layer 2: resolve_exact_category (if provided), on unique unknowns\n",
    "    with log_time(\"S2.3b resolve_exact_category\"):\n",
    "        if \"resolve_exact_category\" in globals():\n",
    "            mask = df_cat[\"category\"].isna()\n",
    "            uniq = df_cat.loc[mask, \"ingredient_key\"].drop_duplicates()\n",
    "            if not uniq.empty:\n",
    "                res = {k: resolve_exact_category(k) for k in uniq}\n",
    "                hit = df_cat[\"ingredient_key\"].map(res)\n",
    "                m = mask & hit.notna()\n",
    "                df_cat.loc[m, \"category\"] = hit[m]\n",
    "                df_cat.loc[m, \"match_method\"] = \"map_resolve\"\n",
    "\n",
    "    # Layer 3: foodcategories.json dict (ingredient-only)\n",
    "    with log_time(\"S2.4 json cache (ingredient only)\"):\n",
    "        if json_map:\n",
    "            mask = df_cat[\"category\"].isna()\n",
    "            if mask.any():\n",
    "                hit = df_cat.loc[mask, \"ingredient_key\"].map(json_map)\n",
    "                m = hit.notna()\n",
    "                idx = hit.index[m]\n",
    "                df_cat.loc[idx, \"category\"] = hit.loc[idx]\n",
    "                df_cat.loc[idx, \"match_method\"] = \"cache_json\"\n",
    "\n",
    "    # Layer 4: plant milk heuristic\n",
    "    with log_time(\"S2.5 milk heuristic\"):\n",
    "        mask = df_cat[\"category\"].isna()\n",
    "        if mask.any():\n",
    "            cand = df_cat.loc[mask, \"ingredient_key\"].map(lambda k: _milk_heuristic(k, allowed_key2disp))\n",
    "            m = cand.notna()\n",
    "            idx = cand.index[m]\n",
    "            df_cat.loc[idx, \"category\"] = cand.loc[idx]\n",
    "            df_cat.loc[idx, \"match_method\"] = \"milk_heuristic\"\n",
    "\n",
    "    # Layer 5: fuzzy to allowed (high cutoff), with vegan guard (dedup pairs)\n",
    "    with log_time(\"S2.6 fuzzy (unique pairs)\"):\n",
    "        mask = df_cat[\"category\"].isna()\n",
    "        fuzzy_df = pd.DataFrame(columns=[\"ingredient_key\",\"product_key\",\"category\",\"match_method\"])\n",
    "        if mask.any():\n",
    "            pairs = df_cat.loc[mask, [\"ingredient_key\",\"product_key\"]].drop_duplicates()\n",
    "            vegan_map = {pk: _vegan_hint(pk) for pk in pairs[\"product_key\"].unique()}\n",
    "            rows = []\n",
    "            for _, r in pairs.iterrows():\n",
    "                ik, pk = r[\"ingredient_key\"], r[\"product_key\"]\n",
    "                best_norm, score = _fuzzy_best(ik, allowed_norm)\n",
    "                if best_norm and score >= FUZZY_CUTOFF:\n",
    "                    cat = allowed_key2disp[best_norm]\n",
    "                    if vegan_map.get(pk, False) and _is_dairy(cat):\n",
    "                        continue\n",
    "                    rows.append({\"ingredient_key\": ik, \"product_key\": pk, \"category\": cat, \"match_method\": \"fuzzy\"})\n",
    "            if rows:\n",
    "                fuzzy_df = pd.DataFrame(rows)\n",
    "                df_cat = df_cat.merge(fuzzy_df, on=[\"ingredient_key\",\"product_key\"], how=\"left\", suffixes=(\"\",\"_fzy\"))\n",
    "                fill = df_cat[\"category\"].isna() & df_cat[\"category_fzy\"].notna()\n",
    "                df_cat.loc[fill, \"category\"] = df_cat.loc[fill, \"category_fzy\"]\n",
    "                df_cat.loc[fill, \"match_method\"] = \"fuzzy\"\n",
    "                df_cat.drop(columns=[c for c in (\"category_fzy\",\"match_method_fzy\") if c in df_cat.columns], inplace=True)\n",
    "                # Persist fuzzy hits to the simple dict (ingredient-only)\n",
    "                # Persist fuzzy hits to the simple dict (ingredient-only)\n",
    "                add_fuzzy = (fuzzy_df[[\"ingredient_key\",\"category\"]]\n",
    "                             .drop_duplicates(\"ingredient_key\"))\n",
    "                appended_json_total += _append_foodcats_map(json_cache_path, add_fuzzy)\n",
    "\n",
    "\n",
    "    # Layer 6: per-run CSV cache\n",
    "    with log_time(\"S2.7 run CSV cache\"):\n",
    "        if not run_cache.empty:\n",
    "            cache_hit = df_cat[[\"ingredient_key\",\"product_key\"]].merge(run_cache, on=[\"ingredient_key\",\"product_key\"], how=\"left\")\n",
    "            has = df_cat[\"category\"].isna() & cache_hit[\"category\"].notna()\n",
    "            df_cat.loc[has, \"category\"] = cache_hit.loc[has, \"category\"]\n",
    "            df_cat.loc[has, \"match_method\"] = cache_hit.loc[has, \"match_method\"].fillna(\"gpt_cached\")\n",
    "\n",
    "    # Layer 7: GPT fallback (dedup pairs; strict to allowed)\n",
    "    with log_time(\"S2.8 GPT (unique unknown pairs; constrained)\"):\n",
    "        need = df_cat[\"category\"].isna() | df_cat[\"category\"].eq(\"unknown\")\n",
    "        if need.any():\n",
    "            if \"_call_gpt\" not in globals():\n",
    "                print(\"[S2.8] WARNING: _call_gpt not defined; skipping GPT fallback.\")\n",
    "                gpt_df = pd.DataFrame(columns=[\"ingredient_key\",\"product_key\",\"category\",\"match_method\"])\n",
    "            else:\n",
    "                pairs = df_cat.loc[need, [\"ingredient_key\",\"product_key\"]].drop_duplicates()\n",
    "                allowed_list = list(allowed_key2disp.values())\n",
    "    \n",
    "                rows = []\n",
    "                for _, r in pairs.iterrows():\n",
    "                    ik, pk = r[\"ingredient_key\"], r[\"product_key\"]\n",
    "                    cat = _call_gpt_categorize(ik, pk, allowed_list, vegan_guard=_vegan_hint(pk))\n",
    "                    if cat:\n",
    "                        rows.append({\n",
    "                            \"ingredient_key\": ik,\n",
    "                            \"product_key\": pk,\n",
    "                            \"category\": cat,\n",
    "                            \"match_method\": \"gpt\"\n",
    "                        })\n",
    "\n",
    "                # ... after building gpt_df ...\n",
    "                gpt_df = pd.DataFrame(rows, columns=[\"ingredient_key\",\"product_key\",\"category\",\"match_method\"])\n",
    "                \n",
    "                # --- After GPT: persist to dict cache (append-only), skipping 'unknown' ---\n",
    "                if not gpt_df.empty:\n",
    "                    _update_run_cache(csv_cache_path, gpt_df)  # keep your per-run cache\n",
    "                    add_gpt = (gpt_df[gpt_df[\"category\"].astype(str).str.lower() != \"unknown\"]\n",
    "                               [[\"ingredient_key\",\"category\"]]\n",
    "                               .drop_duplicates(\"ingredient_key\"))\n",
    "                    # If you added the run counter, capture how many were appended:\n",
    "                    try:\n",
    "                        appended_json_total += _append_foodcats_map(json_cache_path, add_gpt)\n",
    "                    except NameError:\n",
    "                        _append_foodcats_map(json_cache_path, add_gpt)\n",
    "                # -------------------------------------------------------------------------\n",
    "                \n",
    "                # Keep your existing merge-back block unchanged:\n",
    "                if not gpt_df.empty:\n",
    "                    df_cat = df_cat.merge(\n",
    "                        gpt_df, on=[\"ingredient_key\",\"product_key\"], how=\"left\", suffixes=(\"\",\"_gpt\")\n",
    "                    )\n",
    "                    fill = (df_cat[\"category\"].isna() | df_cat[\"category\"].eq(\"unknown\")) & df_cat[\"category_gpt\"].notna()\n",
    "                    df_cat.loc[fill, \"category\"] = df_cat.loc[fill, \"category_gpt\"]\n",
    "                    df_cat.loc[fill, \"match_method\"] = \"gpt\"\n",
    "                    df_cat.drop(columns=[c for c in (\"category_gpt\",\"match_method_gpt\") if c in df_cat.columns], inplace=True)\n",
    "\n",
    "    # Finalize unknowns\n",
    "    with log_time(\"S2.9 finalize unknowns\"):\n",
    "        still = df_cat[\"category\"].isna()\n",
    "        df_cat.loc[still, \"category\"] = \"unknown\"\n",
    "        df_cat.loc[still, \"match_method\"] = df_cat.loc[still, \"match_method\"].fillna(\"unknown\")\n",
    "\n",
    "    # ----------------- Outputs (preserve row_sheet; sorted) -----------------\n",
    "    with log_time(\"S2.A write outputs\"):\n",
    "\n",
    "        # Categories file: keep key columns up front; keep the rest as-is\n",
    "        front = [c for c in [\"row_sheet\",\"product\",\"qty\",\"ingredient\",\"percent\",\"category\",\"match_method\"] if c in df_cat.columns]\n",
    "        rest  = [c for c in df_cat.columns if c not in front and c not in (\"ingredient_key\",\"product_key\")]\n",
    "        df_out = df_cat[front + rest].copy()\n",
    "        if \"row_sheet\" in df_out.columns:\n",
    "            df_out[\"row_sheet\"] = pd.to_numeric(df_out[\"row_sheet\"], errors=\"coerce\")\n",
    "            df_out = df_out.sort_values([\"row_sheet\",\"product\",\"ingredient\"], kind=\"stable\")\n",
    "        out_path = output_dir / Path(f\"categories_{base}.csv\")\n",
    "        df_out.to_csv(out_path, index=False)\n",
    "\n",
    "    with log_time(\"S2.B summary\"):\n",
    "        print(f\"[Stage 2] Wrote: {out_path.name} ({len(df_out):,} rows)\")\n",
    "        if 'appended_json_total' in locals():\n",
    "            print(f\"[S2.cache] foodcategories.json appended {appended_json_total} new key(s) this run.\")\n",
    "\n",
    "    with log_time(\"S2.Z cleanup\"):\n",
    "        try:\n",
    "            if csv_cache_path.exists():\n",
    "                csv_cache_path.unlink()\n",
    "                print(f\"[S2.cache] Deleted per-run cache {csv_cache_path.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[S2.cache] WARN: could not delete {csv_cache_path.name}: {e}\")\n",
    "\n",
    "    return df_out\n",
    "\n",
    "# --- merged categories column order for run_stage_2 (v100f) ---\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "def run_stage_2(*args, **kwargs):\n",
    "    # Policy (v100g):\n",
    "    # - Reorders the categories DataFrame so 'category' appears immediately after 'ingredient'.\n",
    "    # - Persists that order to categories_{BASE}.csv when base is available.\n",
    "    df = run_stage_2_base(*args, **kwargs)\n",
    "    try:\n",
    "        cols = list(df.columns)\n",
    "        if \"ingredient\" in cols and \"category\" in cols:\n",
    "            new_cols = []\n",
    "            for c in cols:\n",
    "                if c == \"ingredient\":\n",
    "                    new_cols.append(\"ingredient\"); new_cols.append(\"category\")\n",
    "                elif c == \"category\":\n",
    "                    continue\n",
    "                else:\n",
    "                    new_cols.append(c)\n",
    "            df = df[new_cols]\n",
    "        base = kwargs.get(\"base\") or kwargs.get(\"base_filename\")\n",
    "        if base is None and len(args) >= 1:\n",
    "            base = args[0]\n",
    "        if base:\n",
    "            df.to_csv(output_dir / Path(f\"categories_{base}.csv\"), index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"[v100f] Could not reorder/persist categories output: {e}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7e5e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 8 — Stage 3: Impact Calculation ===\n",
    "# Purpose: Quantify environmental and animal-welfare impacts for each categorized ingredient.\n",
    "# Inputs:\n",
    "#   - categories_{BASE}.csv (Stage 2 output)\n",
    "#   - factors.csv (impact conversion factors)\n",
    "#   - optional CATEGORY_WEIGHT_MULTIPLIERS (from config)\n",
    "# Outputs:\n",
    "#   - impacts_metric_{BASE}.csv — impacts per kg, m², L, etc.\n",
    "#   - impacts_lbs_{BASE}.csv — impacts per lb, ft², gal, etc.\n",
    "# Behavior:\n",
    "#   - Joins category data with factors table.\n",
    "#   - Applies unit conversions and category multipliers.\n",
    "#   - Aggregates impacts by product and category.\n",
    "# Notes:\n",
    "#   - Requires that categories_{BASE}.csv exists (raises FileNotFoundError otherwise).\n",
    "#   - Respects USE_CATEGORY_WEIGHT_MULTIPLIERS flag.\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re  # (added) for species pattern matching in bone-in shim\n",
    "\n",
    "# ---- Category weight multipliers (toggle + map) ----\n",
    "# Multiplies weights by a category-specific factor in addition to any existing logic.\n",
    "# set in Cell 1 not here: USE_CATEGORY_WEIGHT_MULTIPLIERS = True or set False to disable\n",
    "\n",
    "# Keys are matched using the same normalization your cell uses (via _norm at call-time).\n",
    "\n",
    "USE_CATEGORY_WEIGHT_MULTIPLIERS = bool(globals().get(\"USE_CATEGORY_WEIGHT_MULTIPLIERS\", False))\n",
    "CATEGORY_WEIGHT_MULTIPLIERS = globals().get(\"CATEGORY_WEIGHT_MULTIPLIERS\", {})  # empty if not set\n",
    "\n",
    "def _mult_for_category(cat: str) -> float:\n",
    "    if not USE_CATEGORY_WEIGHT_MULTIPLIERS:\n",
    "        return 1.0\n",
    "    try:\n",
    "        key = _norm(cat)  # uses your notebook normalizer if present\n",
    "    except Exception:\n",
    "        key = \" \".join(str(cat).strip().lower().split())\n",
    "    return CATEGORY_WEIGHT_MULTIPLIERS.get(key, 1.0)\n",
    "\n",
    "\n",
    "def calculate_impacts(df_cat: pd.DataFrame, factors_csv: str, base: str):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      df_cat: categories_{BASE}.csv already loaded into a DataFrame\n",
    "      factors_csv: path to factors.csv\n",
    "      base: BASE string for file naming\n",
    "\n",
    "    Outputs:\n",
    "      Writes impacts_metric_{BASE}.csv and impacts_lbs_{BASE}.csv\n",
    "      Returns (df_lbs_out, df_metric_out) exactly as written to disk (formatted strings)\n",
    "    \"\"\"\n",
    "    LB_TO_KG = 0.45359237\n",
    "    KG_TO_LB = 1.0 / LB_TO_KG\n",
    "\n",
    "    # ---- Helpers ----\n",
    "    def _norm(x: str) -> str:\n",
    "        try:\n",
    "            return norm_text(x)  # use your notebook's normalizer if present\n",
    "        except Exception:\n",
    "            s = str(x).strip().lower()\n",
    "            s = \" \".join(s.split())\n",
    "            return s\n",
    "\n",
    "    # ---- Load/normalize factors ----\n",
    "    fac = pd.read_csv(factors_csv)\n",
    "    if \"category\" not in fac.columns:\n",
    "        fac = fac.rename(columns={fac.columns[0]: \"category\"})\n",
    "    # Coerce factor columns to numeric, create missing as zeros\n",
    "    need_cols = [\"kcal\",\"co2\",\"carbon_opp\",\"d_lives\",\"t_lives\",\"water\",\"water_us\",\"land_metric\",\"land_acres\",\"eutro\"]\n",
    "    for c in need_cols:\n",
    "        if c not in fac.columns:\n",
    "            fac[c] = 0.0\n",
    "        fac[c] = pd.to_numeric(fac[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # Build normalized keys for merge\n",
    "    fac = fac.copy()\n",
    "    fac[\"__category_key\"] = fac[\"category\"].astype(str).map(_norm)\n",
    "\n",
    "    # ---- Normalize df_cat columns and weights ----\n",
    "    cats = df_cat.copy()\n",
    "    cols = {c.lower(): c for c in cats.columns}\n",
    "\n",
    "    if \"category\" not in cols:\n",
    "        raise KeyError(\"categories file is missing a 'category' column.\")\n",
    "    cat_col = cols[\"category\"]\n",
    "    cats[\"__category_key\"] = cats[cat_col].astype(str).map(_norm)\n",
    "\n",
    "    # weight in lbs priority: ingredient_weight_lbs > weight_lbs > product_weight_lbs * percent\n",
    "    def _colnum(name):\n",
    "        return pd.to_numeric(cats[name], errors=\"coerce\") if name in cats.columns else None\n",
    "\n",
    "    wlbs = None\n",
    "    if \"ingredient_weight_lbs\" in cols:\n",
    "        wlbs = _colnum(cols[\"ingredient_weight_lbs\"])\n",
    "    if wlbs is None or wlbs.isna().all():\n",
    "        if \"weight_lbs\" in cols:\n",
    "            wlbs = _colnum(cols[\"weight_lbs\"])\n",
    "    if wlbs is None or wlbs.isna().all():\n",
    "        if (\"product_weight_lbs\" in cols) and ((\"percent\" in cols) or (\"pct\" in cols)):\n",
    "            pct_col = cols.get(\"percent\") or cols.get(\"pct\")\n",
    "            p_wlbs = _colnum(cols[\"product_weight_lbs\"])\n",
    "            pct = pd.to_numeric(cats[pct_col], errors=\"coerce\")\n",
    "            wlbs = p_wlbs * (pct / 100.0)\n",
    "\n",
    "    if wlbs is None:\n",
    "        raise KeyError(\"No usable weight_lbs found (need ingredient_weight_lbs OR weight_lbs OR product_weight_lbs+percent).\")\n",
    "\n",
    "    # ======= BEGIN bone-in edible-weight shim (minimal change; everything else stays v90) =======\n",
    "    # Ensure the flag exists\n",
    "    if \"bone_in\" not in cats.columns:\n",
    "        cats[\"bone_in\"] = False\n",
    "\n",
    "    # Species patterns + yields\n",
    "    EDIBLE_YIELD_DEFAULTS = {\n",
    "        (\"chicken\", False): 1,   (\"chicken\", True): 0.7708,\n",
    "        (\"turkey\",  False): 1,   (\"turkey\",  True): 0.7708,\n",
    "        (\"beef\",    False): 1,   (\"beef\",    True): 0.8667,\n",
    "        (\"pork\",    False): 1,   (\"pork\",    True): 0.8784,\n",
    "        (\"lamb\",    False): 1,   (\"lamb\",    True): 0.6643,\n",
    "        (\"fish\",    False): 1,   (\"fish\",    True): 0.57,  # whole/specimen fish often ~0.5–0.65 edible\n",
    "    }\n",
    "    _SPECIES_PATTERNS = [\n",
    "        (\"chicken\", re.compile(r\"\\bchicken\\b\", re.I)),\n",
    "        (\"turkey\",  re.compile(r\"\\bturkey\\b\",  re.I)),\n",
    "        (\"beef\",    re.compile(r\"\\bbeef|bovine|cow\\b\", re.I)),\n",
    "        (\"pork\",    re.compile(r\"\\bpork|swine\\b\", re.I)),\n",
    "        (\"lamb\",    re.compile(r\"\\blamb|mutton\\b\", re.I)),\n",
    "        (\"fish\",    re.compile(\n",
    "            r\"\\bfish|salmon|tuna|cod|haddock|pollock|anchovy|sardine|halibut|mahi|\"\n",
    "            r\"catfish|tilapia|trout|bass|redfish|snapper|hake|sole|flounder|\"\n",
    "            r\"mackerel|herring\\b\", re.I)),\n",
    "    ]\n",
    "    def _species_key(cat_str: str):\n",
    "        c = str(cat_str or \"\")\n",
    "        for species, rx in _SPECIES_PATTERNS:\n",
    "            if rx.search(c):\n",
    "                return species\n",
    "        return None\n",
    "    def _yield_for_row(cat_str: str, bone_in_flag: bool) -> float:\n",
    "        sp = _species_key(cat_str)\n",
    "        if sp is None:\n",
    "            return 1.0\n",
    "        return EDIBLE_YIELD_DEFAULTS.get((sp, bool(bone_in_flag)), 1.0)\n",
    "\n",
    "    # Apply edible yield row-by-row\n",
    "    # Vectorized edible-yield using existing _SPECIES_PATTERNS + EDIBLE_YIELD_DEFAULTS\n",
    "    cat_txt = cats[cat_col].astype(str).str.lower()\n",
    "#    bone = cats[\"bone_in\"].fillna(False).astype(bool) if \"bone_in\" in cats.columns else pd.Series(False, index=cats.index)\n",
    "    bone = (\n",
    "        cats[\"bone_in\"].astype(str).str.strip().str.lower().eq(\"true\")\n",
    "    ) if \"bone_in\" in cats.columns else pd.Series(False, index=cats.index)\n",
    "\n",
    "    \n",
    "    # Assign first-matching species according to the order in _SPECIES_PATTERNS\n",
    "    species = pd.Series(pd.NA, index=cats.index, dtype=\"object\")\n",
    "    for sp, rx in _SPECIES_PATTERNS:  # reuses your compiled patterns\n",
    "        m = cat_txt.str.contains(rx)\n",
    "        species.loc[species.isna() & m] = sp\n",
    "    \n",
    "    # Build yield maps from the single source of truth (no hardcoded numbers here)\n",
    "    boneless_map = {sp: y for (sp, b), y in EDIBLE_YIELD_DEFAULTS.items() if not b}\n",
    "    bone_map     = {sp: y for (sp, b), y in EDIBLE_YIELD_DEFAULTS.items() if b}\n",
    "    \n",
    "    yield_boneless = species.map(boneless_map).fillna(1.0)\n",
    "    yield_bone     = species.map(bone_map)\n",
    "    \n",
    "    # Choose bone-in yield when bone=True, otherwise boneless; default to 1.0\n",
    "    edible_yield = yield_boneless.where(~bone, yield_bone).fillna(yield_boneless)\n",
    "    \n",
    "\n",
    "    edible_lbs = pd.to_numeric(wlbs, errors=\"coerce\").fillna(0.0) * edible_yield.fillna(1.0)\n",
    "    edible_kg  = edible_lbs * LB_TO_KG\n",
    "    # ======= END bone-in shim =======\n",
    "\n",
    "    # Store per-row edible weights under the same internal names v90 expects\n",
    "    cats[\"__weight_lbs\"] = edible_lbs\n",
    "    cats[\"__weight_kg\"]  = edible_kg\n",
    "\n",
    "    # ---- Apply category multipliers to produce/extend effective weights ----\n",
    "    # Start from existing effective weights if your bone-in/edible-yield logic already created them,\n",
    "    # otherwise start from the base weights. We only apply the category multiplier here to avoid\n",
    "    # touching your existing edible-yield math.\n",
    "    base_lbs = cats[\"__weight_lbs_eff\"] if \"__weight_lbs_eff\" in cats.columns else cats[\"__weight_lbs\"]\n",
    "    base_kg  = cats[\"__weight_kg_eff\"]  if \"__weight_kg_eff\"  in cats.columns else cats[\"__weight_kg\"]\n",
    "    \n",
    "    # Per-row category multiplier\n",
    "    cat_mult = cats[\"category\"].astype(str).map(_mult_for_category) if \"category\" in cats.columns else \\\n",
    "               cats[cat_col].astype(str).map(_mult_for_category)\n",
    "    \n",
    "    cats[\"__weight_lbs_eff\"] = pd.to_numeric(base_lbs, errors=\"coerce\").fillna(0.0) * cat_mult\n",
    "    cats[\"__weight_kg_eff\"]  = pd.to_numeric(base_kg,  errors=\"coerce\").fillna(0.0) * cat_mult\n",
    "\n",
    "\n",
    "    # ---- DEBUG: are edible weights changing? ----\n",
    "    try:\n",
    "        base_sum = float(pd.to_numeric(wlbs, errors=\"coerce\").fillna(0).sum())\n",
    "    except Exception:\n",
    "        base_sum = float(\"nan\")\n",
    "    edible_sum = float(pd.to_numeric(cats[\"__weight_lbs\"], errors=\"coerce\").fillna(0).sum())\n",
    "    print(f\"[debug] base lbs sum={base_sum:,.2f}   edible lbs sum={edible_sum:,.2f}   delta={edible_sum - base_sum:,.2f}\")\n",
    "    \n",
    "    # Check a few chicken rows and their yields/flags\n",
    "    chix = cats[cats[cat_col].astype(str).str.contains(\"chicken\", case=False, regex=True)]\n",
    "    print(f\"[debug] chicken rows={len(chix)}   bone_in True count={int(chix.get('bone_in', pd.Series()).isin([True,'True',1,'1']).sum())}\")\n",
    "    print(chix[[cat_col, \"bone_in\", \"__weight_lbs\"]].head(8).to_string(index=False))\n",
    "\n",
    "    # ---- Group by category ----\n",
    "    grp = (\n",
    "        cats.groupby(\"__category_key\", as_index=False)\n",
    "            .agg(total_weight_lbs=(\"__weight_lbs_eff\", \"sum\"),\n",
    "                 total_weight_kg=(\"__weight_kg_eff\",  \"sum\"))\n",
    "    )\n",
    "\n",
    "\n",
    "    # ---- Merge display names (majority label per key) ----\n",
    "    disp = (\n",
    "        cats.assign(__category_disp=cats[cat_col].astype(str))\n",
    "            .groupby(\"__category_key\")[\"__category_disp\"]\n",
    "            .agg(lambda s: s.value_counts(dropna=False).idxmax())\n",
    "            .reset_index()\n",
    "    )\n",
    "    out = grp.merge(disp, on=\"__category_key\", how=\"left\")\n",
    "    out.rename(columns={\"__category_disp\":\"category\"}, inplace=True)\n",
    "\n",
    "    # ---- Merge factors ----\n",
    "    out = out.merge(fac, on=\"__category_key\", how=\"left\", suffixes=(\"\",\"_f\"))\n",
    "\n",
    "    # ---- Compute impacts (unit aware; see your factor definitions) ----\n",
    "    # Metric-side (per kg factors)\n",
    "    out[\"kcal\"]         = out[\"kcal\"] * out[\"total_weight_kg\"]\n",
    "    out[\"co2_kg\"]       = out[\"co2\"] * out[\"total_weight_kg\"]\n",
    "    out[\"carbon_opp_kg\"]   = out[\"carbon_opp\"] * out[\"total_weight_kg\"]\n",
    "    out[\"water_liters\"] = out[\"water\"] * out[\"total_weight_kg\"]\n",
    "    out[\"land_sqm\"]     = out[\"land_metric\"] * out[\"total_weight_kg\"]\n",
    "    out[\"eutro_g\"]      = out[\"eutro\"] * out[\"total_weight_kg\"]\n",
    "\n",
    "    # Imperial-side composites\n",
    "    out[\"kcal_lbs\"]         = out[\"kcal\"]  # same scalar value, separate column for selection\n",
    "    out[\"co2_lbs\"]          = out[\"co2_kg\"] * KG_TO_LB\n",
    "    out[\"carbon_opp_lbs\"]   = out[\"carbon_opp_kg\"] * KG_TO_LB\n",
    "    # water: prefer water_us (gal/lb) when present (>0 somewhere), else liters→gal\n",
    "    use_gal = (out[\"water_us\"] > 0).any()\n",
    "    out[\"water_gal\"]        = np.where(use_gal,\n",
    "                                       out[\"water_us\"] * out[\"total_weight_lbs\"],\n",
    "                                       out[\"water_liters\"] * 0.264172)\n",
    "    # land: prefer land_acres (acres/lb) when present (>0 somewhere), else sqm→sqft\n",
    "    use_acres = (out[\"land_acres\"] > 0).any()\n",
    "    out[\"land_sqft\"]        = np.where(use_acres,\n",
    "                                       out[\"land_acres\"] * out[\"total_weight_lbs\"] * 43560.0,\n",
    "                                       out[\"land_sqm\"] * 10.7639)\n",
    "    out[\"eutro_lb\"]         = out[\"eutro_g\"] / 453.59237\n",
    "\n",
    "    # Lives (per lb factors)\n",
    "    out[\"d_lives_imp\"] = out[\"d_lives\"] * out[\"total_weight_lbs\"]\n",
    "    out[\"t_lives_imp\"] = out[\"t_lives\"] * out[\"total_weight_lbs\"]\n",
    "\n",
    "    # ---- Build outputs (pre-format numeric frames) ----\n",
    "    metric = out[[\n",
    "        \"category\",\"total_weight_kg\",\"kcal\",\"co2_kg\",\"carbon_opp_kg\",\"d_lives_imp\",\"t_lives_imp\",\"water_liters\",\"land_sqm\",\"eutro_g\"\n",
    "    ]].rename(columns={\"d_lives_imp\":\"d_lives\",\"t_lives_imp\":\"t_lives\"}).copy()\n",
    "\n",
    "    lbs = out[[\n",
    "        \"category\",\"total_weight_lbs\",\"kcal_lbs\",\"co2_lbs\",\"carbon_opp_lbs\",\"d_lives_imp\",\"t_lives_imp\",\"water_gal\",\"land_sqft\",\"eutro_lb\"\n",
    "    ]].rename(columns={\"kcal_lbs\":\"kcal\",\"d_lives_imp\":\"d_lives\",\"t_lives_imp\":\"t_lives\"}).copy()\n",
    "\n",
    "    # ---- Add totals (Dairy & All) BEFORE formatting ----\n",
    "    # Dairy set (normalized)\n",
    "    DAIRY_CATEGORIES = {\n",
    "        \"butter\",\"dairy cheese\",\"cream\",\"dairy milk\",\"buttermilk\",\"ice cream\",\"low fat yogurt\",\n",
    "        \"milk powder\",\"yogurt\",\"concentrated milk\",\"ghee\",\"lactose powder\",\"skim milk\",\"whey powder\"\n",
    "    }\n",
    "    metric[\"__key\"] = metric[\"category\"].map(_norm)\n",
    "    lbs[\"__key\"]    = lbs[\"category\"].map(_norm)\n",
    "\n",
    "    is_dairy = metric[\"__key\"].isin(DAIRY_CATEGORIES)\n",
    "\n",
    "    def _sum_rows(df):\n",
    "        return {\n",
    "            \"total_weight\": pd.to_numeric(df.filter(like=\"total_weight\").sum(numeric_only=True), errors=\"coerce\").max(skipna=True),\n",
    "            \"kcal\": df[\"kcal\"].sum(skipna=True),\n",
    "            \"co2\":  (df[\"co2_kg\"].sum(skipna=True) if \"co2_kg\" in df.columns else df[\"co2_lbs\"].sum(skipna=True)),\n",
    "            \"carbon_opp\": (df[\"carbon_opp_kg\"].sum(skipna=True) if \"carbon_opp_kg\" in df.columns else df[\"carbon_opp_lbs\"].sum(skipna=True)),\n",
    "            \"d_lives\": df[\"d_lives\"].sum(skipna=True),\n",
    "            \"t_lives\": df[\"t_lives\"].sum(skipna=True),\n",
    "            \"water\": df.filter(regex=r\"^water_\").sum(numeric_only=True).max(skipna=True),\n",
    "            \"land\": df.filter(regex=r\"^land_\").sum(numeric_only=True).max(skipna=True),\n",
    "            \"eutro\": df.filter(regex=r\"^eutro_\").sum(numeric_only=True).max(skipna=True),\n",
    "        }\n",
    "\n",
    "    # Metric totals\n",
    "    m_d = metric[is_dairy]\n",
    "    m_a = metric\n",
    "    m_d_s = _sum_rows(m_d)\n",
    "    m_a_s = _sum_rows(m_a)\n",
    "\n",
    "    metric_totals = pd.DataFrame([\n",
    "        {\"category\":\"TOTAL — Dairy\",\n",
    "         \"total_weight_kg\": m_d[\"total_weight_kg\"].sum(skipna=True),\n",
    "         \"kcal\": m_d_s[\"kcal\"], \"co2_kg\": m_d_s[\"co2\"], \"carbon_opp_kg\": m_d_s[\"carbon_opp\"],\n",
    "         \"d_lives\": m_d_s[\"d_lives\"], \"t_lives\": m_d_s[\"t_lives\"],\n",
    "         \"water_liters\": m_d_s[\"water\"], \"land_sqm\": m_d_s[\"land\"], \"eutro_g\": m_d_s[\"eutro\"]},\n",
    "        {\"category\":\"TOTAL — All\",\n",
    "         \"total_weight_kg\": m_a[\"total_weight_kg\"].sum(skipna=True),\n",
    "         \"kcal\": m_a_s[\"kcal\"], \"co2_kg\": m_a_s[\"co2\"], \"carbon_opp_kg\": m_a_s[\"carbon_opp\"],\n",
    "         \"d_lives\": m_a_s[\"d_lives\"], \"t_lives\": m_a_s[\"t_lives\"],\n",
    "         \"water_liters\": m_a_s[\"water\"], \"land_sqm\": m_a_s[\"land\"], \"eutro_g\": m_a_s[\"eutro\"]},\n",
    "    ])\n",
    "\n",
    "    metric = pd.concat([metric.drop(columns=[\"__key\"]), metric_totals], ignore_index=True)\n",
    "\n",
    "    # LBS totals\n",
    "    l_d = lbs[lbs[\"__key\"].isin(DAIRY_CATEGORIES)]\n",
    "    l_a = lbs\n",
    "    l_d_s = _sum_rows(l_d)\n",
    "    l_a_s = _sum_rows(l_a)\n",
    "\n",
    "    lbs_totals = pd.DataFrame([\n",
    "        {\"category\":\"TOTAL — Dairy\",\n",
    "         \"total_weight_lbs\": l_d[\"total_weight_lbs\"].sum(skipna=True),\n",
    "         \"kcal\": l_d_s[\"kcal\"], \"co2_lbs\": l_d_s[\"co2\"], \"carbon_opp_lbs\": l_d_s[\"carbon_opp\"],\n",
    "         \"d_lives\": l_d_s[\"d_lives\"], \"t_lives\": l_d_s[\"t_lives\"],\n",
    "         \"water_gal\": l_d_s[\"water\"], \"land_sqft\": l_d_s[\"land\"], \"eutro_lb\": l_d_s[\"eutro\"]},\n",
    "        {\"category\":\"TOTAL — All\",\n",
    "         \"total_weight_lbs\": l_a[\"total_weight_lbs\"].sum(skipna=True),\n",
    "         \"kcal\": l_a_s[\"kcal\"], \"co2_lbs\": l_a_s[\"co2\"], \"carbon_opp_lbs\": l_a_s[\"carbon_opp\"],\n",
    "         \"d_lives\": l_a_s[\"d_lives\"], \"t_lives\": l_a_s[\"t_lives\"],\n",
    "         \"water_gal\": l_a_s[\"water\"], \"land_sqft\": l_a_s[\"land\"], \"eutro_lb\": l_a_s[\"eutro\"]},\n",
    "    ])\n",
    "\n",
    "    lbs = pd.concat([lbs.drop(columns=[\"__key\"]), lbs_totals], ignore_index=True)\n",
    "\n",
    "    def _round_numeric_only(df: pd.DataFrame, decimals: int = 6) -> pd.DataFrame:\n",
    "        \"\"\"Return a copy of df with numeric columns rounded; non-numeric columns unchanged.\"\"\"\n",
    "        df_rounded = df.copy()\n",
    "        numeric_cols = df_rounded.select_dtypes(include=[\"number\"]).columns\n",
    "        df_rounded[numeric_cols] = df_rounded[numeric_cols].round(decimals)\n",
    "        return df_rounded\n",
    "    \n",
    "    metric = _round_numeric_only(metric, 6)\n",
    "    lbs    = _round_numeric_only(lbs,    6)\n",
    "\n",
    "    # ---- Strict formatting (write numbers as strings with exact decimals) ----\n",
    "    metric_fmt = {\n",
    "        \"total_weight_kg\": \"{:.0f}\",\n",
    "        \"kcal\":            \"{:.0f}\",\n",
    "        \"co2_kg\":          \"{:.0f}\",\n",
    "        \"carbon_opp_kg\":      \"{:.0f}\",\n",
    "        \"d_lives\":         \"{:.1f}\",\n",
    "        \"t_lives\":         \"{:.1f}\",\n",
    "        \"water_liters\":    \"{:.0f}\",\n",
    "        \"land_sqm\":        \"{:.0f}\",\n",
    "        \"eutro_g\":         \"{:.0f}\",\n",
    "    }\n",
    "    lbs_fmt = {\n",
    "        \"total_weight_lbs\": \"{:.0f}\",\n",
    "        \"kcal\":             \"{:.0f}\",\n",
    "        \"co2_lbs\":          \"{:.0f}\",\n",
    "        \"carbon_opp_lbs\":   \"{:.0f}\",\n",
    "        \"d_lives\":          \"{:.1f}\",\n",
    "        \"t_lives\":          \"{:.1f}\",\n",
    "        \"water_gal\":        \"{:.0f}\",\n",
    "        \"land_sqft\":        \"{:.0f}\",\n",
    "        \"eutro_lb\":         \"{:.0f}\",\n",
    "    }\n",
    "\n",
    "    def _apply_fmt_strict(df: pd.DataFrame, spec: dict) -> pd.DataFrame:\n",
    "        out = df.copy()\n",
    "        for col, fmt in spec.items():\n",
    "            if col in out.columns:\n",
    "                vals = pd.to_numeric(out[col], errors=\"coerce\")\n",
    "                out[col] = vals.map(lambda x: \"\" if pd.isna(x) else fmt.format(x)).astype(\"object\")\n",
    "        return out\n",
    "\n",
    "    metric_cols = [\"category\",\"total_weight_kg\",\"kcal\",\"co2_kg\",\"carbon_opp_kg\",\"d_lives\",\"t_lives\",\"water_liters\",\"land_sqm\",\"eutro_g\"]\n",
    "    lbs_cols    = [\"category\",\"total_weight_lbs\",\"kcal\",\"co2_lbs\",\"carbon_opp_lbs\",\"d_lives\",\"t_lives\",\"water_gal\",\"land_sqft\",\"eutro_lb\"]\n",
    "\n",
    "    metric = metric[metric_cols]\n",
    "    lbs    = lbs[lbs_cols]\n",
    "\n",
    "    metric_out = _apply_fmt_strict(metric, metric_fmt)\n",
    "    lbs_out    = _apply_fmt_strict(lbs,    lbs_fmt)\n",
    "\n",
    "    metric_out.to_csv(output_dir / f\"impacts_metric_{base}.csv\", index=False, encoding=\"utf-8\")\n",
    "    lbs_out.to_csv(output_dir / f\"impacts_lbs_{base}.csv\", index=False, encoding=\"utf-8\")\n",
    "    print(f\"[impacts] wrote impacts_metric_{base}.csv and impacts_lbs_{base}.csv\")\n",
    "\n",
    "    return lbs_out, metric_out\n",
    "\n",
    "\n",
    "# --- If your runner calls this function, keep it as-is ---\n",
    "def run_stage_3(base_filename: str, factors_filename: str):\n",
    "    cats_path = output_dir / Path(f\"categories_{base_filename}.csv\")\n",
    "    df_cat = pd.read_csv(cats_path)\n",
    "    return calculate_impacts(df_cat, factors_filename, base_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b2a0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 9 — Runner / Workflow Controller ===\n",
    "# Purpose: Orchestrate execution of all pipeline stages based on START_MODE.\n",
    "# Inputs:\n",
    "#   - input_filename and configuration from Cell 1\n",
    "#   - intermediate files (ingredients_, categories_)\n",
    "# Dependencies:\n",
    "#   - run_stage_1, run_stage_2, run_stage_3 functions\n",
    "# Outputs:\n",
    "#   - Console/log summary of executed stages\n",
    "# Behavior:\n",
    "#   - START_MODE='full' → runs all stages sequentially\n",
    "#   - START_MODE='after_ingredients' → skips Stage 1\n",
    "#   - START_MODE='after_categories' → skips Stage 1 and 2\n",
    "#   - Uses file_type to route to correct Stage 1 function\n",
    "# Error Handling:\n",
    "#   - Raises clear messages if required input files or functions are missing.\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import pandas as pd\n",
    "import inspect  # for safe routing to type03 function\n",
    "\n",
    "def _die(msg: str):\n",
    "    print(f\"[ERROR] {msg}\")\n",
    "    raise SystemExit(\"Fix errors above and re-run this cell.\")\n",
    "\n",
    "# --- Normalize/ensure BASE & base_filename consistency\n",
    "try:\n",
    "    input_filename\n",
    "except NameError:\n",
    "    _die(\"`input_filename` is not defined in the config cell.\")\n",
    "try:\n",
    "    base_filename\n",
    "except NameError:\n",
    "    base_filename = Path(input_filename).stem\n",
    "BASE = Path(input_filename).stem\n",
    "\n",
    "# --- Ensure file_type is resolved (handle 'auto' here before any Stage 1 routing)\n",
    "try:\n",
    "    _ft_lower = str(file_type).lower()\n",
    "except NameError:\n",
    "    _die(\"'file_type' is not defined. Run the config/setup cells first.\")\n",
    "\n",
    "if _ft_lower == \"auto\":\n",
    "    # We rely on Cell 3's helpers:\n",
    "    if 'safe_read_csv' not in globals() or 'autodetect_type' not in globals():\n",
    "        _die(\"Cannot resolve file_type='auto' because safe_read_csv/autodetect_type are undefined. Run Cell 3.\")\n",
    "    _raw_for_detect = safe_read_csv(input_filename)\n",
    "    file_type = autodetect_type(_raw_for_detect)\n",
    "    print(f\"[runner] file_type resolved from 'auto' → '{file_type}'\")\n",
    "\n",
    "# --- Provide robust fallbacks for run_stage_1 / run_stage_2 / run_stage_3\n",
    "if 'run_stage_1' not in globals():\n",
    "    def run_stage_1(input_filename: str, file_type: str, base_filename: str):\n",
    "    # Policy (v100g):\n",
    "    # - Within the SAME kernel session, skip re-running Stage 1 if ingredients_{BASE}.csv was already created.\n",
    "    # - On a NEW session (after kernel restart), allow overwriting older outputs.\n",
    "    # - If 'qty' exists in the returned DataFrame, ensure it is preserved to disk.\n",
    "        if 'load_and_standardize' not in globals():\n",
    "            _die(\"`load_and_standardize` is not defined. Ensure the standardization cell ran.\")\n",
    "        if 'extract_ingredients' not in globals():\n",
    "            _die(\"`extract_ingredients` is not defined. Ensure the ingredient extraction cell ran.\")\n",
    "        std_df = load_and_standardize(input_filename, file_type)\n",
    "        ing_df = extract_ingredients(std_df, base_filename)\n",
    "        return ing_df\n",
    "\n",
    "if 'run_stage_2' not in globals():\n",
    "    def run_stage_2(base: str):\n",
    "        ing_path = output_dir / Path(f\"ingredients_{base}.csv\")\n",
    "        if not ing_path.exists():\n",
    "            _die(f\"`{ing_path.name}` not found. Run Stage 1 first or check `base_filename`/`BASE`.\")\n",
    "        df_ing = pd.read_csv(ing_path)\n",
    "        if 'categorize_ingredients' not in globals():\n",
    "            _die(\"`categorize_ingredients` is not defined. Ensure the categorization cell ran.\")\n",
    "        df_cat = categorize_ingredients(df_ing, base)\n",
    "        return df_cat\n",
    "\n",
    "if 'run_stage_3' not in globals():\n",
    "    def run_stage_3(base: str, factors_csv: str):\n",
    "        cats_path = output_dir / Path(f\"categories_{base}.csv\")\n",
    "        if not cats_path.exists():\n",
    "            _die(f\"`{cats_path.name}` not found. Run Stage 2 first or check `base_filename`/`BASE`.\")\n",
    "        df_cat = pd.read_csv(cats_path)\n",
    "        if 'calculate_impacts' not in globals():\n",
    "            _die(\"`calculate_impacts` is not defined. Ensure the impacts cell ran.\")\n",
    "        return calculate_impacts(df_cat, factors_csv, base)\n",
    "\n",
    "# --- Strict Stage 1 router (type03 requires Cell 5's GPT path)\n",
    "def run_stage_1_router(input_filename: str, file_type: str, base_filename: str):\n",
    "    ft = (file_type or \"\").lower()\n",
    "    if ft == \"type03\":\n",
    "        if 'run_stage_1_type03_gpt' not in globals():\n",
    "            _die(\"type03 file but Cell 5 function not available. Run Cell 5 first.\")\n",
    "        fn = globals()['run_stage_1_type03_gpt']\n",
    "        try:\n",
    "            nparams = len(inspect.signature(fn).parameters)\n",
    "        except Exception:\n",
    "            nparams = 0\n",
    "        print(\"[runner] Stage 1: using type03 GPT path (Cell 5).\")\n",
    "        return fn(input_filename, file_type, base_filename) if nparams >= 3 else fn(input_filename, base_filename)\n",
    "    print(\"[runner] Stage 1: using standard path.\")\n",
    "    return run_stage_1(input_filename, file_type, base_filename)\n",
    "\n",
    "# --- Banner\n",
    "print(\"FoodImpacts — runner\")\n",
    "print(f\"  input_filename: {Path(input_filename).resolve()}\")\n",
    "print(f\"  file_type: {file_type}\")\n",
    "print(f\"  base_filename: {base_filename}\")\n",
    "print(f\"  CATEGORY_SOURCE: {CATEGORY_SOURCE if 'CATEGORY_SOURCE' in globals() else '(unspecified)'}\")\n",
    "print(f\"  START_MODE: {START_MODE}\")\n",
    "\n",
    "# --- Execute requested start mode\n",
    "if START_MODE == \"full\":\n",
    "    print(\"[runner] Stage 1: force recompute ingredients (will overwrite any existing ingredients file).\")\n",
    "    df_ing = run_stage_1_router(input_filename, file_type, base_filename)\n",
    "    df_cat = run_stage_2(base_filename)\n",
    "    df_imp_lbs, df_imp_metric = run_stage_3(base_filename, factors_filename)\n",
    "\n",
    "elif START_MODE == \"after_ingredients\":\n",
    "    df_cat = run_stage_2(base_filename)\n",
    "    df_imp_lbs, df_imp_metric = run_stage_3(base_filename, factors_filename)\n",
    "\n",
    "elif START_MODE == \"after_categories\":\n",
    "    df_imp_lbs, df_imp_metric = run_stage_3(base_filename, factors_filename)\n",
    "\n",
    "else:\n",
    "    _die(\"START_MODE must be 'full', 'after_ingredients', or 'after_categories'\")\n",
    "\n",
    "print(\"\\n[runner] Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
